# Thinking, Fast and Slow

Daniel Kahneman (2011) • Amazon

***

The book "Thinking, Fast and Slow" by Daniel Kahneman explores the two systems of thinking in the human brain: System 1 and System 2. System 1 is fast, automatic, and prone to errors, while System 2 is slow, deliberate, and requires effort. These systems work together but have distinct characteristics and limitations.

System 1 operates automatically and effortlessly, relying on quick judgments, intuition, and unconscious associations. It assesses normality, seeks causes, and can attribute effects without actual causality. However, it can lead to biases and errors, such as the anchoring effect and availability heuristic.

System 2 involves conscious attention and critical thinking. It requires mental effort, and one can only focus on one task. It is responsible for self-control, rational decision-making, and challenging System 1's automatic responses. However, System 2 can become overwhelmed or occupied, leading to reliance on System 1.

The clash between System 1 and System 2 can result in biases and errors in decision-making. Kahneman discusses various biases and heuristics that affect our judgments, including the anchoring effect, availability heuristic, representativeness, and regression to the mean.

The book also delves into the concepts of utility theory and prospect theory. Utility theory suggests that people value outcomes based on utility or satisfaction rather than monetary value. Prospect theory, on the other hand, introduces the role of reference points, diminishing sensitivity to changes in wealth, and an aversion to losses. It highlights how people's preferences and decision-making are influenced by gains and losses relative to reference points.

Kahneman emphasises the importance of adopting risk policies, which provide consistent approaches to decision-making across similar choices. Individuals can make more rational decisions by considering broader frames, such as joint evaluation. The book also explores the impact of memory formation, focusing illusions, affective forecasting, and strategies to overcome biases and improve decision-making.

Overall, "Thinking, Fast and Slow" provides insights into the cognitive processes underlying human judgment and decision-making. It encourages awareness of biases and offers strategies to make more rational choices.

***

## Two Systems

The brain operates using two systems: System 1 and System 2. System 1 is fast and automatic, requiring little effort, while System 2 is slow and consumes mental energy. Both systems work together.

System 1 performs automatic activities such as judging distances, solving simple math equations, recognising images, completing phrases, responding to sounds, and executing instinctual actions like swatting mosquitoes. It includes innate skills shared with animals and acquired abilities through practice and learning. System 1 actions are often involuntary and effortless.

System 2, on the other hand, involves mental events that require conscious attention. Examples include focusing on a specific voice in a noisy room, searching for a person with particular characteristics in a crowd, maintaining an uncomfortable walking speed, filling out complex forms, recalling specific memories, preparing for physical actions, and monitoring social interactions. Engaging in System 2 activities demands mental energy and typically allows for only one task at a time, as the intense focus can make one oblivious to other stimuli.

System 1 handles automatic and effortless mental processes, while System 2 requires deliberate attention and mental effort. They complement each other but have distinct characteristics and limitations.

### System 1

System 1 is characterised by immediate responses prone to errors due to insufficient information. However, these rapid judgments are crucial for human survival, enabling us to react quickly in potentially dangerous situations. System 1 assesses normality, seeks causes and intentions, and can attribute effects without actual causality. It relies on unconscious associations and patterns to make sense of the world around us. The "halo effect" exemplifies System 1's tendency to form sweeping conclusions based on limited information. Priming effects also operate within System 1, where exposure to ideas or stimuli influences our subsequent behaviour. For instance, voters are more likely to support increased school budgets if they vote in a school building, showcasing the influence of priming.

### System 2

System 2 requires significant effort and reduces awareness of other stimuli, even those typically capturing our attention. This system is associated with physical changes like pupil dilation and increased heart rate, which Kahneman used to study the transition between mental processes. System 2 operates slower, and engaging it in rapid tasks depletes cognitive resources quickly.

When the mind is intensely focused on System 2 tasks, such as complex calculations, it becomes cognitively busy. This cognitive busyness makes individuals more susceptible to making superficial judgments and impulsive decisions because System 2 is occupied, leaving System 1 to take over. However, System 2 is responsible for critical thinking, self-control, and rational decision-making, representing genuine cognitive aptitude.

### Clashes Between System 1 and System 2

System 1 constantly monitors our surroundings and engages in basic assessments, believing and confirming what it observes, while System 2 doubts and challenges. System 1 cannot solely focus on the assigned task from System 2 and inevitably performs other assessments simultaneously.

When an intuitive answer is not readily available, System 1 generates a more straightforward question, substitutes it, and answers it. This process, known as a heuristic, helps find answers to difficult questions, even if the solutions may not be perfect. However, issues arise when the generated heuristics are inadequate substitutes. The mood is heuristic involves substituting an assessment of the current mood for more complex evaluations of general happiness or emotional states. The affect heuristic refers to how our likes and dislikes influence our beliefs about the world.

Cognitive ease is when things are going well, and System 2 intervention is unnecessary. Cognitive strain, on the other hand, occurs when unmet needs require System 2's effort. Mere familiarity can deceive the brain into believing something is true, as the ease of comprehension influences our perception of truth. Conversely, cognitive strain engages System 2 when encountering information that is difficult to process.

Mood influences intuition, as good mood and cognitive ease are linked, while bad mood and cognitive strain tend to go together. Happy perspectives may enhance creativity but also increase the likelihood of logical errors.

System 1 operates automatically, seeking patterns and making quick judgments, which can be prone to errors in complex situations. System 2 doubts and challenges but needs to be more active, often relying on information provided by System 1, even when System 2 should take over.

## Heuristics and Biases

Human beings, including trained experts, must become more naturally adept at statistics. Our automatic thought process, System 1, tends to perceive relationships and causality even when they do not exist, resulting in a tendency towards unwarranted confidence instead of doubt.

### The Anchoring Effect

The anchoring effect refers to the tendency of individuals to be influenced by an initial value when estimating an unknown number. Even if the initial value is random and unrelated, subsequent estimates tend to be "anchored" to it. For instance, when considering the purchase of a car listed at $40,000 and receiving a 10 per cent discount, one might feel they are getting a good deal by paying $36,000. However, this perception could be attributed to anchoring to the initial price of $40,000, which may have no relevance to the car's actual value.

### Availability

The availability heuristic refers to our tendency to estimate the prevalence or frequency of something based on how easily we can recall examples of it and how recently we have been exposed to it. For example, if you have recently heard about several famous musicians dying at a young age, you might be inclined to believe that a significant portion of musicians die young.

This heuristic introduces a bias in how we perceive and assess risks. Reporting trends can influence what chances we perceive more prominently, leading us to attach greater importance to them. However, one type of risk may be far less significant than another. System 1, responsible for sensing danger, responds to these perceived trends. Unfortunately, our actions and responses may be inappropriate or ineffective due to availability bias.

### Representativeness

Humans often rely on representativeness rather than statistical base rates when predicting probabilities. This occurs when System 1 substitutes heuristic questions for actual calculations of likelihood based on base rates, leading to intuitive judgments. However, anchoring our estimates to reliable diagnostic evidence and base rates is more accurate.

Statistical base rates provide information about the characteristics of populations but do not specifically describe the facts of individual cases. When forming judgments and predictions, we undervalue statistical base rates and give more weight to causal base rates, even though both are equally relevant. This process is influenced by the stereotypes ingrained in System 1. Bayesian analysis is a statistical tool that can help mitigate these effects by working with base rates and removing intuition from the process. This theorem allows users to input known data and base rates to arrive at logical predictions.

### The Linda Problem

Mistaking plausibility for probability is a standard error made by System 1. Kahneman highlights this through the example of the Linda problem, an experiment he conducted with his partner. In this scenario, participants are asked whether it is more likely for Linda to be a bank teller or a feminist bank teller. Statistically, the correct answer is that she is more likely to be a bank teller since all feminist tellers are bank tellers. However, many people incorrectly choose "feminist bank teller" as the answer. System 1, whose role is to assess plausibility, is deceived by the scenario, and the "feminist bank teller" feels more plausible to System 1. By selecting this answer, individuals decrease their chances of being correct.

### Regression to the Mean

Regression to the mean refers to the phenomenon where extreme outcomes, whether exceptionally good or bad, are likely to be followed by less severe consequences. For instance, if a fighter pilot has a terrible flight during training, their subsequent flight will probably be better, regardless of any praise or punishment received.

While most psychologists have found that praise is generally more effective for training over time, many military trainers believe that criticism and punishment yield better results. This belief stems from observing a feeble flight followed by immediate punishment and a subsequent improvement in performance. However, statistically speaking, the better flight was highly likely to occur regardless of the sentence, as the initial poor flight was simply a regression to the mean.

It is important to note that System 1, a cognitive trait, tends to rely on weak evidence to make extreme or rare predictions. It often substitutes heuristic or simplified questions and answers in complex situations, avoiding thorough analysis and data-driven decision-making.

## Overconfidence

Our expectations for the future are heavily influenced by the narratives we construct about the past, but these narratives often need to be completed and biased due to hindsight. This cognitive illusion, the narrative fallacy, leads to outcome bias, where poor decisions with positive outcomes are mistakenly considered good decisions. For example, investors and pundits may believe they have the skill to make successful predictions despite the lack of concrete evidence when in reality, much of their success is based on luck.

Algorithms outperform human intuition, even that of experts, in predicting outcomes. Experts can contradict their evaluations when presented with the exact case materials in different contexts, about 20%, highlighting System 1's context-dependent influence.

True expertise requires a predictable environment and the opportunity to learn the factors that make it predictable. Sufficient practice and exposure can develop expertise to the point where complex assessments become automatic, such as a chess master instantly knowing a game's best moves and outcome. For most people, such calculations would require the engagement of System 2.

The planning fallacy describes unrealistic predictions that are overly optimistic, while reference-case forecasting, anchoring predictions to statistics from similar cases, can improve accuracy. The sunk-cost error occurs when people persist with plans despite evidence suggesting they should be abandoned, often due to an unwillingness to admit defeat or lose money already invested.

Entrepreneurial delusions involve unrealistic assessments of success and an illusion of control, neglecting the impact of competitors.

It is essential to adopt algorithms in decision-making processes to improve decision-making outcomes. This may involve learning to use tools like Bayes's theorem for predictions or anchoring assessments to realistic base rates and data sources rather than unrelated anchors or intuition.

## Choices

This section focuses on the impact of System 1 and System 2 on our decision-making related to wealth, risk, and loss. Kahneman explores how these cognitive systems influence our perception of risks, losses, and accumulating and losing wealth.

### Utility Theory

Bernoulli's utility theory suggests that risk-averse individuals prefer an inevitable outcome of lesser value over an uncertain one, essentially paying a cost to avoid uncertainty. According to this theory, the utility or satisfaction derived from wealth, rather than the money itself, brings happiness. However, Kahneman challenges this theory by emphasising the importance of reference points. He argues that individuals who have experienced only adverse outcomes are more likely to take risks, even when the practical numerical outcome is the same as risk-averse individuals.

To illustrate this point, consider a scenario where you and I each have one million dollars. According to utility theory, we should both be equally happy because we possess the same ability to achieve things with our wealth. However, this overlooks the influence of our reference points. If you had ten million dollars yesterday while I had only one thousand, chances are you are far less happy with your current situation than my increased satisfaction.

Kahneman highlights the significance of reference points in shaping our perception of wealth and happiness, suggesting that utility theory alone does not capture the full complexity of human decision-making.

### Prospect Theory

In contrast to utility theory, prospect theory incorporates three cognitive aspects that influence our decision-making. Firstly, evaluations are made relative to a neutral reference point. Secondly, there is a diminishing sensitivity to changes in wealth, meaning the impact of each subsequent change is less significant than the initial change. Lastly, a shared aversion to losses is a fundamental component of prospect theory. Additionally, prospect theory acknowledges the endowment effect, where the value of something increases once it is possessed, based on a reference point.

According to Kahneman, the critical insights from prospect theory are the existence of reference points and the asymmetry between our perception of losses and gains. Losses are perceived as more significant than equivalent gains. This understanding explains why people tend to fight harder to avoid losses compared to their efforts to achieve gains. Furthermore, individuals have a sense of unfairness or entitlement, leading them to believe their losses are unjust.

Prospect theory recognises the importance of reference points, the influence of the endowment effect, and the tendency for losses to significantly impact our decision-making more than gains. These insights show why people are motivated to prevent losses and may perceive failures as unfair.

### The Fourfold Pattern

Kahneman and Tversky introduced the fourfold pattern, which explains a system of preferences based on two key ideas. Firstly, individuals assign value not to overall wealth but to gains and losses. Secondly, people assign decision weights to outcomes that deviate from actual probabilities. These concepts give rise to four distinct scenarios:

1. The possibility effect occurs When confronted with a low probability gain, leading to risk-seeking behaviour. Individuals may mistake something only possible for being probable, leading them to engage in actions like buying a lottery ticket despite it being a poor financial choice.

2. When faced with a low probability loss, the possibility effect induces risk-averse behaviour. Individuals may opt for insurance coverage to mitigate potential losses, even though the chances of experiencing the loss are low.

3. In situations involving a gain with a high probability, the certainty effect influences risk-averse behaviour. Individuals tend to opt for a "sure thing" in the present, such as receiving $80 today, rather than waiting for a highly "likely" $100 in the future.

4. When dealing with a high probability loss, the certainty effect leads to risk-seeking behaviour. Instead of cutting their losses, individuals may engage in gambling or risky behaviour, hoping to reverse the situation.

The fourfold pattern highlights how people's preferences and decision-making vary depending on the probability and magnitude of gains and losses they face. It provides insights into risk-seeking and risk-averse behaviours observed in different scenarios.

### Rare Events

During decision-making, individuals tend to assign greater significance and higher probabilities to unlikely events. Factors such as vividness of events, obsessive concerns, explicit reminders, and tangible representations contribute to this phenomenon. For instance, many people in the United States today fear a terrorist act more than a car accident or a heart attack, despite the statistically higher risk associated with the latter two events. This bias occurs because terrorist attacks are more vivid and receive explicit media coverage daily, making us perceive them as more probable and deserving of greater importance in our decision-making.

### Making Decisions

Developing a risk policy that can be consistently applied is more beneficial than evaluating each risky choice individually. A risk policy involves considering a single risky option within the context of similar decisions, avoiding biases like optimism and narrow framing that can lead to overly risk-averse choices. By implementing a risk policy, reasonable approaches to risks can be applied across the board rather than on a case-by-case basis.

For instance, an investor who reacts strongly to daily fluctuations in the stock market is likely to make poor investment decisions. Every slight loss or gain may heavily impact their judgment, disregarding larger patterns. In this case, a risk policy could be to evaluate their portfolio only once every quarter, enabling them to make decisions based on overall trends rather than daily fluctuations.

Another example is deciding whether to purchase an extended warranty. If a person recently had a negative experience with a device, they might be inclined to pay a high premium for an extended warranty. However, statistically, the likelihood of needing another assurance may be very low, especially if they have never needed one. In this scenario, a risk policy would suggest not buying extended warranties, preventing emotional decisions driven by a recent negative experience.

Rational decision-making is focused on future consequences and does not justify past actions. However, at least partly, humans tend to base decisions on positions that validate previous choices and avoid decisions that may induce regret.

Comprehensive and broader frames like joint evaluation are more conducive to rational decision-making. By considering multiple factors and comparisons, decisions can be made more logically. Proper framing of questions helps individuals see the bigger picture and make informed choices. Placing experiences within the larger context of one's life reduces the influence of emotions and facilitates more rational decision-making.

Applying risk policies consistently allows each risky choice to be evaluated within the context of similar decisions, avoiding biases and promoting more rational decision-making. Broad frames like joint evaluation help create risk policies that facilitate sound decision-making.


## Two Selves

The formation of memories is influenced by factors such as primacy, recency, intensity, and frequency. This means that people remember the beginning and end of an experience more vividly and are affected by its emotional power and frequency. As a result, someone married for twenty years but had a terrible experience in the last two years may perceive the entire marriage as terrible, neglecting the overall duration. This phenomenon, known as duration neglect, can impact rational decision-making.

Focusing illusions and affective forecasting also contribute to judgment errors. We often mistakenly believe that acquiring material possessions will bring us happiness, only to realise that joy is short-lived or elusive. Furthermore, our current mood can influence how we recall and assess memories. For example, we are more likely to negatively evaluate job satisfaction or marriage on a lousy day. Linking mood and memory can lead us to misinterpret our needs and wants, resulting in suboptimal actions.

While humans are not inherently irrational, we require assistance making better decisions and forming accurate judgments. We also need strategies to counteract those who exploit the vulnerabilities of both System 1 and System 2 and their interactions. Some advertisers take advantage of our impulsive decision-making and emotional responses. To navigate these cognitive traps, striving for more rational decision-making as a regular practice is crucial.

To avoid errors associated with System 1, it is essential to recognise when we are in cognitive danger, slow down our thought process, and engage System 2 for assistance in making more informed choices.

Various factors influence our memory formation, and judgment errors can occur due to focusing illusions, affective forecasting, and exploiting our decision-making weaknesses. By being aware of these tendencies and actively working towards rational decision-making, we can improve our judgments and make more optimal choices.
