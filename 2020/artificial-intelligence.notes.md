
# Artificial Intelligence: A Guide for Thinking Humans
Author: Melanie Mitchell

## Prologue: Terrified
- AI and GEB:
  - Melanie reflects on the pace of computer advancement and acknowledges that computers can't appreciate irony.
  - Shares an incident of getting lost in Googleplex, Google's headquarters, highlighting the irony.
  - Recalls the creation of Google's AI research group, Google Brain, and the acquisition of AI start-ups.
  - Google's founders, Larry Page and Sergey Brin, are motivated to create artificial intelligence.
  - Discusses the growth of Google from a search portal to an applied AI company.
  - Google's aspiration, as expressed by its DeepMind group, is to "Solve intelligence and use it to solve everything else."
  - Melanie is excited to attend an AI meeting at Google, inspired by her passion for AI since graduate school and admiration for Google's accomplishments.
  - The meeting is organized to have AI researchers converse with Douglas Hofstadter, the AI legend and author of "Gödel, Escher, Bach."
- Chess and the First Seed of Doubt:
  - Describes Hofstadter's initial skepticism about the feasibility of AI, how AI evolved over the years, and his work on simulating intelligence.
  - Hofstadter's speculation in the 1970s about the emergence of AI, with critics like John Searle suggesting its impossibility.
  - Hofstadter's earlier belief that AI's realization was distant and not a significant concern.
  - Mention of Hofstadter's ten questions and speculations about AI in "Gödel, Escher, Bach."
  - Hofstadter's initial belief that computers would not achieve excellence in chess, a stance that was proven wrong due to the rapid improvement in chess programs.
  - Explanation of the capabilities of human chess players to recognize patterns and strategic configurations.
  - Hofstadter's realization of the rapid progress in computer chess programs and their mastery through brute-force approaches.
- Music: The Bastion of Humanity:
  - Hofstadter's conviction that certain human qualities, like creating beautiful music, are beyond the reach of AI.
  - Music as a language of emotions, requiring complex emotions and understanding, not achievable by AI.
  - Introduction to the program EMI (Experiments in Musical Intelligence) by composer David Cope.
  - EMI's ability to compose music in the style of various classical composers by following a set of composition rules.
  - Hofstadter's distress upon hearing EMI's compositions, as it challenged his perception of the depth and humanity of music.
  - Hofstadter's belief that AI's ability to replicate music could cheapen the essence of human expression.
- Google and the Singularity:
  - Hofstadter's apprehensions about Google's AI efforts, including self-driving cars, speech recognition, and computer-generated art.
  - Google's association with Ray Kurzweil and his concept of the Singularity, where AI outpaces human intelligence.
  - Hofstadter's doubts about the Singularity's feasibility, despite its influence on Google's endeavors.
  - Hofstadter's fear that rapid AI advancement might leave humanity behind, making humans relics.
- Why Is Hofstadter Terrified?:
  - Hofstadter's fear is rooted in his belief that AI might oversimplify human qualities and experiences.
  - His worry that AI might trivialize complex aspects of humanity, reducing them to algorithms.
  - Hofstadter's concern about AI mechanizing what he values most in humanity, eroding the deep essence of emotions and creativity.
- I Am Confused:
  - Melanie's reflection on the diverse views within the AI field regarding its progress and potential.
  - The contrasting opinions of AI researchers and prominent figures, both optimistic and cautious.
  - Melanie's personal confusion and questioning of her own stance after attending the Google AI meeting.
  - The need to critically examine the progress and potential of AI based on expert opinions.

## Background

### The Roots of Artificial Intelligence
- Origins of AI: The Dartmouth Workshop
  - The origins of artificial intelligence (AI) can be traced back to a workshop held at Dartmouth College in the summer of 1956.
  - The workshop, which lasted for two months and involved ten men, marked the beginning of AI as a field of research and exploration.
- Defining AI and Approaches
  - The participants of the Dartmouth workshop aimed to define AI and explore the possibility of creating machines with human-like intelligence.
  - The early days of AI were marked by a sense of enthusiasm and optimism as researchers tried to define the goals of AI and develop methods to achieve them.
- Anarchy of Methods and Symbolic AI
  - The initial AI research phase was characterized by an "anarchy of methods," with researchers pursuing diverse approaches to achieve artificial intelligence.
  - The symbolic AI approach emerged, focusing on using symbolic representations and logical reasoning to emulate human intelligence.
  - Symbolic AI researchers aimed to create systems that could process symbols and manipulate them according to predefined rules to solve problems.
- Subsymbolic AI: Perceptrons
  - An alternative approach to AI, known as subsymbolic AI, was introduced through the concept of perceptrons.
  - Perceptrons were simple computational units inspired by neurons and capable of learning from data to make decisions or recognize patterns.
  - These perceptrons were initially designed to mimic the human brain's basic processing units, and they used numerical weights and a threshold to make decisions.
- Perceptron's Inputs and Learning
  - The inputs to a perceptron included numerical values that were weighted and combined to produce an output decision.
  - The perceptron learned its weights and threshold values through a learning algorithm, which involved adjusting these parameters based on the error in its predictions compared to the actual outcomes.
- Learning the Perceptron's Weights and Threshold
  - The perceptron-learning algorithm allowed the system to iteratively adjust its weights and threshold using training examples.
  - The perceptron could then use these learned parameters to make accurate decisions on new, unseen examples.
  - Perceptrons could be extended to recognize multiple classes by having separate outputs for each class and learning appropriate weights for each output.
- Limitations of Perceptrons
  - Despite their potential, perceptrons had limitations in terms of the complexity of tasks they could handle.
  - Marvin Minsky and Seymour Papert's book "Perceptrons" mathematically demonstrated the limitations of single-layer perceptrons and the challenges of scaling up to complex problems.
  - Multilayer neural networks, perceptrons with additional layers, were proposed as a solution, but learning algorithms for these networks were lacking at the time.
- AI Winter
  - The field of AI went through a cyclic pattern of optimism and disappointment, often referred to as "AI spring" followed by "AI winter."
  - These cycles were characterized by periods of promising breakthroughs, followed by unmet expectations and decreased funding.
  - Negative assessments and speculations led to reduced government and venture capital funding for AI research.
  - The field experienced repeated cycles of enthusiasm, followed by setbacks and slower research progress.
- Realizations from AI Winters
  - AI winters taught researchers important lessons about the complexity of achieving artificial intelligence.
  - Marvin Minsky's observation that "easy things are hard" highlighted that tasks seemingly simple for humans were challenging for AI systems.
  - The goals of natural language understanding, visual perception, and intuitive learning proved more difficult for AI than tasks like complex problem-solving and game-playing.

## Neural Networks and the Ascent of Machine Learning
- Multilayer Neural Networks and Learning via Back-Propagation
  - Multilayer neural networks, initially dismissed, have become the foundation of modern artificial intelligence.
  - These networks consist of interconnected units that process data in layers, similar to perceptrons.
  - Each layer in a network contains units (or neurons) that compute activations based on weighted inputs.
  - Hidden units, non-output units, play a crucial role in recognizing abstract features beyond simple inputs like pixels.
  - Learning in neural networks involves adjusting weights to minimize errors between predicted and actual outputs.
  - The back-propagation algorithm was developed to propagate error information backward through the network, adjusting weights accordingly.
  - Back-propagation allows networks to gradually modify weights to reduce error and improve accuracy on training examples.
  - While there's no guarantee of exact convergence, back-propagation has shown success in training multilayer networks.
  - Multilayer networks can learn complex patterns and generalize better than simple perceptrons.
- Connectionism and the Resurgence of Neural Networks
  - Connectionism, also known as neural network research, gained traction in the 1980s.
  - Connectionist networks were considered a form of subsymbolic AI, with knowledge stored in weighted connections between units.
  - Connectionist networks aimed to emulate aspects of human learning, perception, and language development.
  - These networks were constructed as scientific models to study how learning occurs in the human brain.
  - While connectionist networks didn't achieve human-level performance, they attracted attention and funding.
- Bad at Logic, Good at Frisbee
  - Symbolic systems, rooted in human-understandable reasoning, excel in tasks requiring logical reasoning and high-level language-like descriptions.
  - Subsymbolic systems, such as neural networks, are better suited for perceptual or motor tasks that lack well-defined rules.
  - Subsymbolic systems excel at tasks like recognizing faces, identifying voices, and other automatic processes.
  - Subsymbolic systems are challenging to interpret and hard to directly program with complex human knowledge.
- The Ascent of Machine Learning
  - The field of machine learning emerged, drawing from statistics and probability theory.
  - Machine learning became a distinct subdiscipline, separate from symbolic AI.
  - Researchers developed algorithms that allow computers to learn from data, rejecting traditional symbolic AI methods.
  - Machine learning went through cycles of optimism, funding, overpromising, and setbacks.
  - The internet's growth provided more data and computing power, setting the stage for the next AI revolution.

## AI Spring
- Spring Fever in AI:
  - The emergence of AI spring, marked by rapid advancements and excitement.
  - Speculation on whether this marks the harbinger of a future AI explosion or is just a waypoint in gradual progress.
  - Questions regarding the potential for achieving human-level AI in the near future.
- AI: Narrow and General, Weak and Strong:
  - Distinction between narrow AI, designed for specific tasks, and general AI, capable of versatile human-like intelligence.
  - Differentiating between weak AI, limited to specific domains, and strong AI, possessing human-level cognitive capabilities.
- Could Machines Think?:
  - Pondering the concept of machine intelligence and whether it can truly replicate human cognitive processes.
  - Raising questions about whether machines could possess consciousness, emotions, and self-awareness similar to humans.
- The Turing Test:
  - Introduction of the Turing Test, proposed by Alan Turing, as a measure of machine intelligence.
  - Description of the test involving human judges interviewing both machines and humans to determine if the machines can convincingly mimic human responses.
- The Singularity:
  - Discussion of the Singularity, a theoretical point where AI surpasses human intelligence, triggering rapid and transformative changes.
  - Ray Kurzweil's optimism about the Singularity, based on exponential progress in computation, neuroscience, and nanotechnology.
- An Exponential Fable:
  - The story of the sage and the chessboard, illustrating exponential growth with each square representing double the grains of rice.
  - Applying exponential growth to computer progress, exemplified by Moore's law in chip components doubling every 1-2 years.
- Exponential Progress in Computers:
  - Moore's law, proposed by Gordon Moore, predicting exponential reduction in computer chip components' size and cost.
  - Kurzweil's belief in exponential technological progress, exemplified by computer speed and memory improvements.
  - Envisioning AI achieving human brain capability by 2023 based on these trends.
- Neural Engineering:
  - Kurzweil's concept of reverse engineering the brain to replicate human-level AI.
  - Disagreements among neuroscientists about the feasibility of this approach due to limited understanding of the brain's complexities.
- Singularity Skeptics and Adherents:
  - Diverse responses to Kurzweil's predictions, ranging from enthusiastic embrace to dismissive skepticism.
  - Influence of Kurzweil's ideas on the tech industry, where some see exponential progress as a solution to societal challenges.
- Wagering on the Turing Test:
  - Introduction of Long Bets, a platform for futurists to make accountable predictions.
  - Mitchell Kapor's bet against Kurzweil, wagering that no machine will pass the Turing Test by 2029.
  - Kurzweil's confidence in AI's potential, emphasizing exponential growth and virtual reality's role in providing human-like experiences.

## Looking and Seeing

### Who, What, When, Where, Why
- Easy Things Are Hard (Especially in Vision)
  - Object recognition in visual data has been a challenging problem in AI research since the 1950s.
  - The task seemed straightforward but has proven difficult for computers due to the complexity of visual scenes and the diversity of object appearances.
  - AI researchers struggled with achieving human-level object recognition, especially when dealing with variations in lighting, pose, and background.
- The Deep-Learning Revolution
  - The breakthrough in object recognition came with the advent of deep learning in the 2010s.
  - Deep learning involves training deep neural networks with multiple hidden layers.
  - Deep neural networks have a structure that mimics parts of the brain's visual system, leading to significant success in various AI tasks, particularly in computer vision.
- The Brain, the Neocognitron, and Convolutional Neural Networks
  - Neuroscientists Hubel and Wiesel's discoveries in the 1950s about hierarchical organization in the brain's visual cortex revolutionized understanding of object recognition.
  - The neocognitron, a deep neural network, was developed based on these findings by Kunihiko Fukushima in the 1970s.
  - The concept of convolutional neural networks (ConvNets) was introduced in the 1980s by Yann LeCun, drawing inspiration from the neocognitron.
  - ConvNets became a driving force in the deep-learning revolution, particularly in computer vision.
- Object Recognition in the Brain and in ConvNets
  - The brain's visual cortex is hierarchically organized with layers of neurons detecting increasingly complex features.
  - Early layers detect edges, while higher layers recognize shapes, objects, and even faces.
  - ConvNets are designed similarly, with successive layers performing convolutions to detect features, creating hierarchical activation maps.
- Input and Output
  - A ConvNet takes an input image, represented as an array of pixel values, and produces an output classification.
  - The goal is to assign a confidence percentage to different categories based on what the network has learned.
- Activation Maps
  - Activation maps in ConvNets are grids of units that act as detectors for specific visual features.
  - These maps are created by convolving the input image with filters to highlight certain features like edges, corners, and shapes.
  - Each layer's maps feed into the next layer, forming a hierarchical representation of visual features.
- Classification in ConvNets
  - After processing the input through convolutional layers, the features are fed into a traditional neural network called the classification module.
  - The classification module outputs confidence percentages for different object categories, allowing the network to predict the image's content.
- Training a ConvNet
  - ConvNets learn from training examples by adjusting their weights to minimize errors in classification.
  - Training involves passing images through the network, comparing predicted and correct labels, and updating weights using back-propagation.
  - Multiple epochs of training are required to improve the network's performance, and it converges when its weights stabilize.

### ConvNets and ImageNet
- ConvNets and their Impact on Computer Vision:
  - Yann LeCun's work on Convolutional Neural Networks (ConvNets) has been instrumental in shaping the field of computer vision.
  - ConvNets emerged from a history of neural network research and led to breakthroughs in image recognition tasks.
  - LeCun's collaboration with Geoffrey Hinton and others played a crucial role in developing ConvNets and pushing their capabilities.
- Building ImageNet and Harnessing Mechanical Turk:
  - Fei-Fei Li and collaborators conceived ImageNet, a large-scale image database structured according to WordNet nouns.
  - They used Amazon Mechanical Turk, a human intelligence marketplace, to sort out relevant images from noisy search results.
  - The collaboration with Mechanical Turk allowed the rapid labeling of millions of images with corresponding WordNet nouns.
- The ImageNet Competitions:
  - The ImageNet Large Scale Visual Recognition Challenge was launched to spur progress in object recognition algorithms.
  - Competing programs were given labeled training images and a list of possible categories to classify images correctly.
  - The challenge featured thousands of categories, and ConvNets started to dominate the competition.
  - The success of ConvNets in the ImageNet competition marked a turning point in computer vision.
- The ConvNet Gold Rush:
  - ConvNets' success in ImageNet spurred a surge in applying them to various real-world applications.
  - Companies like Google, Microsoft, and Facebook integrated ConvNets into products for image search, facial recognition, and more.
  - GPUs played a crucial role in accelerating ConvNet training, leading to the rise of companies like NVIDIA.
- Have ConvNets Surpassed Humans at Object Recognition?
  - Claims that machines have surpassed humans in object recognition need careful consideration.
  - ImageNet evaluation uses a top-5 accuracy metric, where machines' top five guesses are compared to human-provided labels.
  - Human performance is often assessed based on a single human's performance, like Andrej Karpathy's experiment.
  - ConvNets' accuracy on top-1 (single guess) and top-5 accuracy can differ significantly, raising questions about true human-machine comparison.
- Beyond Object Recognition:
  - Computer vision extends beyond object recognition to understanding object relationships, interactions, and scene description.
  - ConvNets need to progress in comprehending actions, emotions, and other complexities in visual scenes.
  - Achieving true visual intelligence requires integration with language and deeper knowledge.

### A Closer Look at Machines That Learn
- Learning on One's Own and Big Data:
  - Humans often assume that others share similar experiences, knowledge, and values, forming a "theory of mind."
  - This theory of mind is not applicable to AI systems like deep networks, leading to trust issues.
  - "Explainable AI," "transparent AI," or "interpretable machine learning" aims to make AI decisions understandable to humans.
  - Researchers visualize features learned by convolutional neural networks (ConvNets) and identify input parts affecting decisions.
  - While explainable AI is advancing, deep networks explaining themselves in human terms remains a challenge.
  - Learning from big data is a strength, but it doesn't equate to human-like learning.
- The Long Tail and Understanding What the Network Learned:
  - ConvNets like AlexNet perform well but don't possess human-like understanding.
  - ConvNets can be easily fooled into misclassifications by adversarial examples, raising questions about their true learning.
  - These networks lack context-dependent understanding and feedback connections that may prevent vulnerabilities.
  - Researchers debate if deep learning exhibits "true understanding" or merely learns from data without grasping concepts.
- Biased AI and Challenges with Explainability:
  - ConvNets may achieve high accuracy but fail unexpectedly due to overfitting, long-tail effects, and susceptibility to hacking.
  - Decisions made by deep networks are often difficult to comprehend, leading to challenges in predicting and addressing failures.
  - Researchers aim to enhance the reliability and transparency of deep networks but face ongoing challenges.
  - Biased AI is a concern, as networks can reinforce biases present in training data.
- Fooling Deep Neural Networks and Vulnerabilities:
  - ConvNets are vulnerable to adversarial attacks, where small changes in inputs lead to incorrect decisions.
  - Researchers created adversarial examples to trick ConvNets into confident misclassifications.
  - Various attacks demonstrated the susceptibility of deep networks in computer vision and language tasks.
  - Defending against these attacks and understanding what networks learn are active research areas.
  - The analogy of "Clever Hans" questions whether deep learning shows genuine understanding or just responds to cues.
  - The success of deep networks coexists with their limitations, making decisions about applying AI challenging.

### On Trustworthy and Ethical AI
- Beneficial AI:
  - AI systems already bring significant benefits to society.
  - AI technology is integral to various services like speech transcription, navigation, email filtering, translation, fraud detection, etc.
  - AI assists creative professionals, students, scientists, visually and hearing impaired individuals.
  - AI applications are expanding into healthcare, scientific modeling, climate change analysis, and space exploration.
- The Great AI Trade-Off:
  - Andrew Ng's analogy: "AI is the new electricity."
  - AI's potential to transform multiple industries and aspects of daily life.
  - Challenges of AI's unpredictable errors, bias, vulnerability to hacking, and lack of transparency.
  - Debate on allowing AI extensive autonomy vs. human oversight.
  - Pew Research Center's study: Divergent opinions on whether AI will enhance human capacities by 2030.
  - Ethical concerns include privacy, civil rights, autonomous weapons, biased decisions, media manipulation, and cybercrime.
- The Ethics of Face Recognition:
  - Face recognition involves labeling faces in images or videos.
  - Applications range from identifying people in photos to law enforcement and surveillance.
  - Privacy concerns arise due to unauthorized tagging and recognition.
  - Accuracy and biases in face recognition systems pose challenges.
  - Face recognition companies market their technology to various industries.
  - Errors and biases in face recognition technology can have serious real-world consequences.
  - Tech companies' responses to concerns include opposition to using face recognition for law enforcement.
- Regulating AI:
  - The need for AI regulation due to risks and ethical concerns.
  - AI challenges like trustworthiness, transparency, bias, and security.
  - Importance of including diverse perspectives in AI regulation.
  - Comparison to the regulation of other technologies, like genetic engineering and medical sciences.
  - Current regulatory efforts by state governments and non-profit organizations.
  - Development of ethical infrastructure for AI research and applications.
- Moral Machines:
  - Historical discussion of machine morality, including Isaac Asimov's "Three Laws of Robotics."
  - Challenges of programming ethics into machines and value alignment.
  - The trolley problem as a symbolic example of ethical programming for self-driving cars.
  - People's differing responses to moral dilemmas in relation to AI ethics.
  - Difficulty in making human values clear and consistent for AI systems.
  - Suggestions to teach machines moral values by observing human behavior.
  - The need for AI systems to possess general common sense for trustworthy moral reasoning.

## Learning to Play

### Rewards for Robots
- Training with Reinforcement Learning:
  - Amy Sutherland's observation on the simple method of rewarding behavior for animal training.
  - Applying the same techniques to train her husband effectively using rewards.
  - The concept of operant conditioning, a psychological technique used on animals and humans.
  - Introduction to reinforcement learning, a machine learning approach inspired by operant conditioning.
  - Distinction between reinforcement learning and supervised learning methods.
  - Reinforcement learning involves an agent performing actions in an environment and receiving rewards as feedback.
  - Rewards in reinforcement learning guide the learning process, similar to rewards in animal training.
- Training Your Robo-Dog:
  - Exotic animal trainers' simple method: "reward behavior I like, ignore behavior I don't."
  - Similar technique applied to training humans, as exemplified by Amy Sutherland's husband.
  - Operant conditioning: reward-based learning method used on animals and humans.
  - Reinforcement learning in AI: contrasts with supervised learning, relies on rewards for learning.
  - Agents (learning programs) perform actions in an environment and receive rewards intermittently.
  - Example: Amy Sutherland's husband receives rewards like smiles, kisses, and praise.
  - AI version of rewards involves adding positive numbers to a machine's memory.
  - Reinforcement learning's history and overshadowing by neural networks.
  - 2016 milestone: reinforcement learning's role in AI beating humans in the game of Go.
  - Overview of reinforcement learning: learning through actions, rewards, and environment interaction.
  - Illustrative example using a robotic soccer game (Aibo robots).
  - Traditional AI approach involves explicit rules for actions like taking steps and kicking.
  - Limitations of manual rule-based approaches for complex tasks.
  - Promise of reinforcement learning: learning flexible strategies by performing actions and receiving rewards.
  - Introduction to "Rosie," a simple agent for reinforcement learning.
  - Rosie's ability to estimate distance from a soccer ball (state).
  - Rosie's actions: Forward, Backward, Kick.
  - Introduction to the Q-table: a table storing action values for each state.
  - Rosie's learning process: receiving rewards, updating Q-table values, making better choices.
  - Crucial notion of action value: reflection of the agent's prediction of future rewards.
  - Value learning through trial and error, exploration vs. exploitation balance.
  - Q-learning algorithm: assigning values to actions in a given state.
  - Training Rosie to perform a soccer task through episodes of learning.
  - Issues omitted from the example: uncertainty in perception, action outcomes, and balance.
  - Ongoing research topics: handling uncertainty, exploration-exploitation balance.
- Stumbling Blocks in the Real World:
  - Challenges in extrapolating the "training Rosie" example to real-world reinforcement learning.
  - Complexity of real-world tasks: defining states and Q-table not feasible.
  - Self-driving cars facing infinite possible states.
  - Transition to neural networks for value assignment in complex tasks.
  - Difficulty of carrying out learning process with real robots due to time and risks.
  - Simulation-based approach for learning in controlled environments.
  - Transferring learning from simulation to real-world contexts varies in success.
  - Successes of reinforcement learning in perfectly simulated domains like game playing.
  - Introduction to the upcoming topic of reinforcement learning applied to games.

### Game On
- Deep Q-Learning:
  - The focus on creating an AI program to play Go effectively has been present since the early days of AI.
  - Deep Q-Learning, a reinforcement learning method, has been a significant approach for training AI agents to play complex games.
- The $650 Million Agent:
  - Reference to AlphaGo, a groundbreaking AI program developed by DeepMind.
  - AlphaGo's significance and impact on AI research and game-playing capabilities.
- Checkers and Chess:
  - Mention of the historic achievements in game-playing AI, including checkers and chess.
  - Highlighting the challenges and successes of AI programs like Samuel's checkers player and Deep Blue in chess.
- Deep Blue:
  - In 1997, Deep Blue defeated Garry Kasparov, a world chess champion, demonstrating AI's potential in game-playing.
- The Grand Challenge of Go:
  - Go's complexity presented a significant challenge for AI due to its immense branching factor and lack of a clear evaluation function.
  - Go's complexity posed difficulties beyond what chess presented for AI.
- AlphaGo Versus Lee Sedol:
  - AlphaGo's impressive victory over Lee Sedol, a top Go player, in a five-game match.
  - The surprise and admiration elicited by AlphaGo's moves and gameplay.
- How AlphaGo Works:
  - AlphaGo employs a combination of deep Q-learning and Monte Carlo tree search.
  - Monte Carlo tree search assigns scores to possible moves based on simulated roll-outs.
  - Unlike traditional brute-force methods, AlphaGo's approach uses Monte Carlo tree search for its evaluation.
  - AlphaGo uses deep convolutional neural networks to guide its move selection and enhance its intuition.
  - The neural network learns patterns from self-play games and is improved by feedback from Monte Carlo tree search.
  - The AI's intuition, aided by Monte Carlo tree search, is developed through this combination of techniques.
  - AlphaGo's training involved millions of self-play games and updating neural network weights based on Monte Carlo tree search outcomes.

### Beyond Games
- Generality and "Transfer Learning":
  - The concept of transfer learning in AI refers to the ability of a program to apply what it has learned in one task to help it perform another related task.
  - Human beings naturally exhibit transfer learning, where skills learned in one context can be applied to others.
  - Current AI systems, including deep reinforcement learning ones, struggle with transfer learning between related tasks.
- "Without Human Examples or Guidance":
  - DeepMind's reinforcement learning approach aims to enable programs to learn on their own through actions and observations without human guidance.
  - AlphaGo, especially the AlphaGo Zero version, demonstrated that it's feasible to achieve superhuman performance without using human-provided examples for learning.
  - However, some aspects critical to AlphaGo's success, like network architecture, were innately built by programmers and not learned from data.
- The Most Challenging of Domains:
  - Assessing a domain's challenge for AI involves comparing the performance of simple algorithms on it.
  - Some seemingly simple tasks for humans can be challenging for computers, while computers excel at tasks humans find challenging.
  - Uber AI Labs showed that simple algorithms like random search and genetic algorithms could match or surpass deep Q-learning's performance on several Atari video games.
  - Games that humans find difficult, like charades, pose even more significant challenges for AI.
- What Did These Systems Learn?:
  - Understanding what deep learning neural networks in game-playing systems have learned is complex and often misunderstood.
  - Attribution of human-like concepts to AI systems is a form of overattribution; they learn specific contingencies rather than genuine understanding.
  - Transfer tests show that deep reinforcement learning's solutions are often superficial and lack the ability to generalize to slight variations of the same game.
- How Intelligent Is AlphaGo?:
  - AlphaGo's achievement in playing Go at a superhuman level is impressive, but it does not possess human-like general intelligence.
  - Unlike humans, AlphaGo's skills are not transferable to other tasks; it doesn't learn to "think" better about anything other than Go.
  - Human intelligence involves flexible thinking and reasoning, not just excelling at a single task.
- From Games to the Real World:
  - Transitioning from AI success in games to real-world applications faces challenges.
  - Games provide well-defined rules, clear reward functions, and limited actions, while the real world is complex and lacks clearly defined "states."
  - Real-world tasks require object recognition, reasoning, planning, and dealing with uncertainties, all of which are more complex than game-playing.
  - Simulating real-world tasks for training is challenging, as the simulation must accurately model physical forces and uncertainties.

## Artificial Intelligence Meets Natural Language

### Words, and the Company They Keep
- Story: The Restaurant:
  - The concept of understanding language and meaning is likened to a restaurant, where various aspects represent different dimensions of meaning.
  - Words like charm, wit, humor, bracelet, and jewelry are compared in a semantic space, highlighting their proximity in meaning along distinct dimensions.
- The Subtlety of Language:
  - Language's complexity is emphasized through its intricate relationships and multiple dimensions of meaning.
  - The challenge of capturing the richness of language in numerical form is discussed.
- Speech Recognition and the Last 10 Percent:
  - Achieving accurate speech recognition involves addressing the final nuances, or "last 10 percent," of understanding.
  - Improvements in speech recognition could enhance communication systems, making them more versatile and human-like.
- Classifying Sentiment:
  - Sentiment analysis involves determining the emotional tone of a piece of text, a key component of Natural Language Processing (NLP) applications.
  - Classifying sentiment can be challenging due to linguistic subtleties and context-dependent interpretations.
- Recurrent Neural Networks:
  - Recurrent Neural Networks (RNNs) are introduced as a solution for processing sequential data in NLP.
  - The concept of word vectors is linked to the ability of RNNs to capture relationships between words in sentences.
- A Simple Scheme for Encoding Words as Numbers:
  - The notion of representing words as numbers is introduced, where each word corresponds to a vector of coordinates in a semantic space.
  - This representation contrasts with the simple one-hot encoding approach, offering enhanced performance for NLP tasks when used as inputs for neural networks.
- The Semantic Space of Words:
  - The "semantic space" represents word meanings using word vectors in multi-dimensional space.
  - The meaning of a word is determined by its position and relationships with other words in this space.
- Word2Vec:
  - Word2Vec, introduced by Google researchers, uses neural networks to learn word vectors by training on vast sets of text data.
  - Word2Vec's training data comprises pairs of words occurring in proximity in documents, and the method predicts word co-occurrences.
  - The network's hidden units capture contextual information, resulting in learned word vectors with dimensions reflecting captured nuances.
- Using Word Vectors for Understanding:
  - Word vectors' distances reflect semantic relationships, enabling analogy-solving tasks (e.g., "Man is to woman as king is to queen").
  - Word2Vec's regularity even captures relationships between countries and their capital cities.
- Dealing with Biases:
  - Word vectors inherit biases present in training data and language, potentially reinforcing societal biases.
  - Analogy problems reveal biases, such as associating "computer programmer" with "man" and "homemaker" with "woman."
  - Efforts to "de-bias" word vectors are ongoing, aiming to mitigate biases in NLP applications.
- The Future of NLP:
  - NLP systems extensively use word vectors, and they are a fundamental element in tasks ranging from speech recognition to language translation.
  - Scientists are investigating biases within word vectors and their potential implications on NLP outputs.

### Translation as Encoding and Decoding
- Encoder, Meet Decoder
  - Google Translate and similar systems provide rapid translations across multiple languages.
  - Online translation systems operate 24/7 and have improved over the years.
  - Google Translate saved time by helping with translations during a stay in France.
  - Early AI efforts in translation relied on human-specified rules.
  - Statistical machine translation emerged in the 1990s, using data-driven approaches.
  - Google Translate transitioned to neural machine translation in 2016.
  - Neural machine translation employs encoder-decoder networks.
  - Recurrent neural networks (RNNs) encode source sentences, producing hidden-unit activations.
  - Decoder networks generate translated sentences using recurrent connections.
  - Long short-term memory (LSTM) units address issues with traditional RNNs.
  - Deep learning, data, and computation have propelled automated machine translation.
- Evaluating Machine Translation
  - Claims of machine translation approaching human levels are based on two evaluation methods.
  - The BLEU automated method compares machine translations with human translations.
  - BLEU has limitations and may overrate poor translations.
  - Gold-standard evaluation involves human raters comparing machine and human translations.
  - Google and Microsoft used human evaluations to show improvements in neural machine translation.
  - Averages and ratings can be misleading indicators of translation quality.
  - Longer passages and nuances challenge machine translation systems.
- Lost in Translation
  - Google Translate's translations demonstrate challenges and limitations.
  - Ambiguities, idiomatic phrases, and contextual nuances lead to translation errors.
  - Machine translation lacks genuine understanding of the text's content.
  - Language and idiomatic subtleties pose significant problems for accurate translation.
  - Despite improvements, machine translations remain fundamentally unreliable.
- Translating Images to Sentences
  - The concept of translating images into language captions is explored.
  - Google's Show and Tell system pairs images with captions using encoder-decoder networks.
  - Training data consists of images paired with multiple captions produced by humans.
  - Image-captioning system uses convolutional neural networks (ConvNets) for encoding.
  - Decoder network generates captions based on the ConvNet activations.
  - Automated image-captioning can be accurate but also exhibits errors.
  - Captioning systems lack genuine understanding of image content.
  - Fundamental lack of understanding in captioning networks limits their reliability.

### Ask Me Anything
- USS Enterprise. Stardate: 42402.7
  - The narrative begins with a reference to USS Enterprise and stardate 42402.7, introducing a sci-fi context that segues into discussions of AI and natural language processing.
- The Story of Watson
  - The journey into AI advancements begins with a recounting of IBM's Watson, known for its triumph in the Jeopardy! competition in 2011.
  - Watson's prowess in natural language processing and its implications for question-answering are outlined.
- Reading Comprehension
  - The challenge of reading comprehension for AI systems is highlighted, acknowledging OpenAI's GPT-3 as a prominent example.
  - The focus shifts to SQuAD, a dataset designed to test question-answering capabilities.
  - The potential of AI systems to generate coherent human-like text is mentioned.
- What Does It Mean?
  - The focus turns to the Allen Institute for Artificial Intelligence's contribution, a dataset of science questions for elementary and middle school levels.
  - An example question is presented to illustrate the complexity of comprehension required.
  - The notion of understanding subtle word differences and common-sense reasoning is introduced.
  - Researchers aim to develop NLP systems that go beyond mere answer extraction and truly understand the context, requiring integration of natural-language processing, background knowledge, and commonsense reasoning.
  - An example of a challenging question involves understanding which simple machine is used when hitting a softball with a bat.
- Understanding Sentences
  - Demonstrates challenges in understanding sentences that differ only by a single word, showcasing the need for background knowledge and commonsense reasoning.
  - Examples include interpreting pronouns like "they," comparisons of age or quantity, and understanding spatial relationships.
- Winograd Schemas
  - Winograd schemas are tests designed to gauge a machine's ability to understand and reason with sentences, often requiring commonsense knowledge.
  - These schemas are named after Terry Winograd, who proposed them to challenge machines' comprehension abilities.
  - While some progress has been made, current machine performance on Winograd schemas is still below human accuracy.
- Adversarial Attacks on Natural-Language Processing Systems
  - Building on the concept of adversarial attacks in computer vision, their extension to NLP systems is explained.
  - A specific example illustrates how image captions can be manipulated to deceive AI systems.
  - The same approach is applied to speech-recognition systems, raising concerns about potential malicious uses.
  - The distinction between "white hat" and "black hat" attackers is established, with implications for system vulnerabilities.
- Concerns and Limitations
  - While deep learning has shown progress in various NLP tasks, achieving human-level language understanding remains challenging.
  - Language comprehension relies on commonsense knowledge, context, and understanding of social dynamics, which machines lack.
  - Current NLP systems are increasingly prevalent, but their lack of humanlike understanding can make them brittle, unreliable, and susceptible to attacks.

## The Barrier of Meaning

### On Understanding
- The Building Blocks of Understanding:
  - Humans possess a foundational body of core knowledge, including basic common sense acquired early in life.
  - Babies develop insights into the cause-and-effect structure of the world, intuitive physics, biology, and psychology.
  - Intuitive physics involves understanding how objects behave, intuitive biology pertains to living things, and intuitive psychology focuses on sensing and predicting people's feelings and beliefs.
  - These core bodies of intuitive knowledge form the basis for human cognitive development, enabling learning, thinking, and making sense of situations.
- Predicting Possible Futures:
  - Understanding a situation involves the ability to predict likely future events based on mental models of how the world works.
  - Mental models allow simulations of situations, and predicting outcomes is based on these models.
  - Mental models enable imagining different possible futures and using them to guide decisions and actions.
- Understanding as Simulation:
  - Psychologist Lawrence Barsalou suggests that understanding is a form of simulation, where mental simulations are performed subconsciously.
  - Mental simulations underlie comprehension of situations we encounter and even those we observe but don't directly participate in.
  - Barsalou's theory suggests that mental simulations of our own experiences contribute to understanding complex concepts and situations.
- Metaphors We Live By:
  - Linguist George Lakoff and philosopher Mark Johnson propose that abstract concepts are understood through metaphors based on core physical knowledge.
  - Everyday language is rich in metaphors that reflect physical concepts, showing how we conceptualize abstract ideas in terms of concrete experiences.
  - Metaphorical concepts reveal the physical basis of our understanding of abstract concepts, supporting Barsalou's theory of simulation.
- Abstraction and Analogy:
  - Abstraction involves recognizing specific instances as part of a broader category, applicable from infancy to complex situations.
  - Abstraction is closely linked to analogy making, which involves perceiving common essences between different things.
  - Analogy making is a fundamental process that underlies our ability to abstract and form concepts, enabling language and thought.

### Knowledge, Abstraction, and Analogy in Artificial Intelligence
- Core Knowledge for Computers
  - The quest for imbuing computers with core human-like knowledge and abilities is a fundamental challenge in artificial intelligence (AI).
  - One significant challenge is to provide computers with an understanding of basic concepts and commonsense knowledge that humans possess.
- Abstraction, Idealized
  - Abstraction is a pivotal cognitive ability, enabling humans to recognize patterns and generalize concepts across various domains.
  - An idealized form of abstraction is sought after in AI, allowing computers to identify higher-level concepts from diverse instances.
- Active Symbols and Analogy Making
  - Active symbols, as employed in Hofstadter's architecture, are essential for building AI systems capable of making analogies.
  - Analogy making involves mapping relationships between different situations, as seen in the Copycat program's letter-string domain.
  - The concept of "successorship" is crucial for recognizing sequences and creating analogies in AI programs.
- Metacognition in the Letter-String World
  - Human intelligence includes metacognition, the ability to perceive and reflect upon one's own thought processes.
  - Metacat, a program developed by James Marshall, aimed to enable self-perception in Copycat, allowing it to reflect on its problem-solving process.
  - Copycat's lack of metacognition sometimes led it to repetitive and unsuccessful paths in problem-solving.
- Recognizing Visual Situations
  - Efforts are being made to develop AI systems, such as Situate, that can flexibly recognize complex visual situations involving multiple entities and relationships.
  - Situate combines object-recognition capabilities of deep neural networks with active-symbol architecture to recognize instances of various situations.
  - The challenge lies in recognizing entire situations, which is more complex than identifying individual objects.
- "We Are Really, Really Far Away"
  - The dominance of deep learning in AI, with its neural networks and big data, faces a barrier called the "barrier of meaning" in achieving robust and general intelligence.
  - Andrej Karpathy's observations highlight the immense disparity between human understanding and computer vision capabilities.
  - Recognizing scenes involves complex inferences about objects, relationships, emotions, and contextual knowledge, far beyond current AI capabilities.

### Questions, Answers, and Speculations
- Question: How soon will self-driving cars be commonplace?
  - There are six levels of vehicle autonomy defined by the U.S. National Highway Traffic Safety Administration.
  - The levels range from human-driven (Level 0) to fully autonomous (Level 5).
  - The challenges for self-driving cars include handling various circumstances and scenarios.
  - Current cars on the road are mainly between levels 0 and 1, with a few at levels 2 and 3.
  - Self-driving car industry aims for full autonomy (Level 5), but challenges like edge cases, security, and malicious attacks persist.
  - The idea of geofencing (allowing autonomous cars only in specific areas) is suggested to address safety concerns.
- Question: Will AI result in massive unemployment for humans?
  - The impact of AI on employment is uncertain, with predictions ranging from job replacement to new job creation.
  - The vulnerability of driving jobs due to autonomous vehicles is discussed, but the timeline is unclear.
  - AI's effect on employment is part of ongoing discussions on AI ethics.
  - Historical examples show that new technologies can create as many new jobs as they replace.
- Question: Could a computer be creative?
  - The possibility of computers being creative is explored.
  - Computers can generate things beyond what their programmers explicitly thought of.
  - Examples of computer-generated art and music are given.
  - Creativity involves understanding and judgment, which current computers lack.
- Question: How far are we from creating general human-level AI?
  - Achieving general human-level AI (strong AI) is far from being achieved.
  - The complexity of human intelligence is underestimated, and current AI is limited.
  - Predictions about the timeline for achieving human-level AI vary widely.
  - The challenges of general AI involve understanding human abilities, consciousness, and more.
- Question: How terrified should we be about AI?
  - The fear of superintelligent AI that rivals human creativity is not imminent.
  - Concerns about AI should focus on its misuse, bias, and limitations.
  - AI's potential for generating fake media is worrisome.
  - A balance is needed between AI's capabilities and its limitations.
- Question: What exciting problems in AI are still unsolved?
  - The majority of important questions in AI remain open and waiting for new ideas.
  - Early AI research focused on natural language processing, neural networks, machine learning, abstraction, and creativity.
  - The complexity of human intelligence makes AI research challenging.
  - The most exciting questions in AI involve both applications and scientific inquiries about intelligence.
