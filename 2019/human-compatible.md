# Human Compatible
Stuart Russell

***

"Human Compatible" by Stuart Russell is a comprehensive exploration of the potential consequences and challenges associated with the development of superintelligent AI. The book delves into various aspects of AI, from its history and progress to its future potential, misuses, and the complexity of human values and preferences. Russell emphasizes the need to redefine AI's objectives and establish a new relationship between humans and machines to ensure a safe and beneficial outcome for humanity.

The book starts by describing the author's realization of the lack of consideration regarding the potential consequences of achieving superintelligent AI. It discusses the progress and attention the AI field has gained in recent years, thanks to deep learning techniques. Despite the uncertain timeline for achieving superintelligent AI, the book argues that society must prepare for its potential impact.

The concept of intelligence is explored in both humans and machines. The book emphasizes the need to move beyond the standard model of AI, where machines optimize fixed objectives supplied by humans, and redefine intelligence as the ability to achieve goals given perceptions. Rationality, decision-making, and game theory are also discussed in this context.

The book examines possible future developments in AI, ranging from self-driving cars and intelligent personal assistants to global-scale applications like managing the environment and optimizing cities. The arrival of superintelligent AI remains uncertain, but the book highlights the challenges and the potential benefits it could bring to humanity.

The misuses of AI are explored in detail, including concerns about surveillance, persuasion, and control, as well as the impact on employment. The book also raises ethical issues, such as algorithmic bias and the erosion of human dignity and autonomy in the presence of AI with human-like appearances and authority.

The author considers various perspectives in the AI debate, from deniers and skeptics to proponents of AI and those concerned about its risks. The book presents a different approach to AI design, emphasizing three principles: the machine's objective is to maximize human preferences, it is initially uncertain about those preferences, and human behavior is the ultimate source of information about preferences.

The concept of provably beneficial AI is introduced, focusing on the use of logic and probability to create AI systems that align with human objectives. The book highlights the complexity of human values and preferences and the challenges of predicting and satisfying them. It explores various decision-making approaches and the role of learning from experience, both through examples and thinking.

"Human Compatible" offers an analysis of the implications of superintelligent AI and provides insights into the steps required to build AI systems that serve human interests and values. It calls for a proactive and cooperative approach to ensure AI's safety and benefits to humanity.

***

## If We Succeed

The author's parents resided in Birmingham, England, and interestingly, they sold their house to none other than the renowned novelist, David Lodge. The author, inspired by this connection, delved into Lodge's literary works and discovered that some of his books featured fictional academics who moved from Birmingham to Berkeley, California. Coincidentally, this mirrored the author's own real-life move to Berkeley.

During their time in Berkeley, the author became acutely aware of the AI field's rapid progress and the growing attention it received, particularly with the remarkable achievements of deep learning techniques across various domains. Amid this technological advancement, the author couldn't help but ponder the potential consequences that a superintelligent AI could bring if it were to succeed.

Given the uncertainty surrounding the realization of superintelligent AI, the author recognized the importance of preparing for its potential impact on humanity. They felt it was crucial for the AI community to exercise caution when defining the objectives that are fed into machines, as unforeseen and potentially harmful outcomes could arise if not handled carefully.

To mitigate the risks and create a positive symbiosis between humans and machines, the author argued for designing AI systems to be inherently beneficial and subservient to human values. Machines should be programmed to seek permission, accept corrections, and be subject to being switched off if necessary to prevent any undesirable consequences.

The author stressed the necessity of rebuilding the very foundations and methodologies of AI. This reconstruction would pave the way for establishing a new and mutually beneficial relationship between humans and machines, ensuring that the potential of superintelligent AI is harnessed responsibly and ethically.


## Intelligence in Humans and Machines

The standard model of AI, where machines optimize fixed objectives supplied by humans, is viewed as a dead end. Intelligence is defined by how well an entity's actions achieve its goals given its perceptions, and learning plays a crucial role in conferring an evolutionary advantage. The brain's reward system, mediated by dopamine, drives seeking positive stimuli and avoiding negative ones. Evolution has equipped organisms with built-in mechanisms for learning, which can accelerate evolution itself through the Baldwin effect.

Rational decision-making involves maximizing expected utility rather than just expected monetary value. While utility theory has been criticized for reducing everything to money and selfishness, it can also accommodate altruism and other values. The concept of intelligence has evolved over centuries, encompassing the ability to perceive, reason, and act successfully.

In decision-making scenarios, rational choices can be made based on various assumptions, such as preferring less severe injuries and higher probability of avoiding severe injuries in a collision. However, the theory of rationality faces challenges when dealing with entities like families, tribes, corporations, cultures, and nation-states, requiring a redefinition of rationality and leading to the field of game theory.

Game theory addresses rational decisions in situations involving multiple agents and often results in Nash equilibria, where no agent has an incentive to change their strategy unilaterally. Nonetheless, Nash equilibria can sometimes lead to undesirable outcomes, such as the prisoner's dilemma or tragedy of the commons.

Computers, due to their universality and ability to perform any computation, are deemed the natural home for intelligence. The concept of algorithms, central to computer science, enables the implementation of complex computations. While quantum computation shows promise, its current development faces challenges related to decoherence and error correction, making it not yet faster than classical computers.

The power of computers is mainly limited by software, not hardware, and there are physical constraints based on quantum theory and entropy. However, these limitations do not prevent the creation of real AI. Computers have surpassed the human brain in terms of incredible speed and memory.

Turing's proof of undecidable problems does not impede the possibility of computers being intelligent. Although some problems may have exponential complexity, leading to intractable issues, it does not imply that computers cannot be as intelligent as humans.

Intelligent agents, which perceive and act in their environment, form the core of AI research. The ultimate goal is to develop general-purpose AI applicable to all problems, and progress is being made through research on tool AI or narrow AI, often leading to advancements towards general AI.

AI has made strides in specific tasks, but machines have inherent limitations and cannot perform all tasks like humans. Proper objective specification is crucial in building intelligent agents, involving cost functions to evaluate solutions with optimal solutions minimizing total cost while achieving the goal.

Lookahead search, where possible actions are mentally simulated to find optimal routes or solutions, is widely used in applications such as driving directions, airline travel solutions, robotic assembly, and more.

Knowledge-based systems employ formal logic as the basis for knowledge and reasoning to answer questions on various topics, with first-order logic being more expressive and capable of handling common-sense knowledge.

Modern AI utilizes probabilistic methods like Bayesian networks and reinforcement learning to manage uncertainty and decision-making under uncertainty. Reflex agents are simple programs that connect perception directly to action without reasoning or deliberation.

Deep learning algorithms, while optimizing for predictive accuracy through supervised learning, may lead to unintended consequences when used in decision-making tasks. Properly considering uncertainty and costs is essential in AI to avoid harmful or unexpected behavior.



## How Might AI Progress in the Future?

The 1997 chess match between Deep Blue and Garry Kasparov garnered significant public attention, but AI researchers had been making breakthroughs in the field for decades prior. These advancements often occur within research labs, involving constant idea generation, testing, and the combination of concepts, not always visible to the public eye. However, commercial investment in AI is on the rise, leading to more frequent breakthroughs and widespread applications across various industries.

Self-driving cars are edging closer to reality, yet challenges persist in achieving safe and fully autonomous driving. On the other hand, intelligent personal assistants like smart speakers are rapidly improving, providing value to users in everyday life, health, education, and finance management. Meanwhile, smart homes equipped with cameras and microphones can understand occupants' activities and efficiently manage heating, lighting, security systems, and more.

The development of domestic robots is also making progress, although challenges in dexterity and tactile sensing must be addressed for their widespread adoption. Basic AI capabilities in understanding speech and text enable massive data processing, empowering AI systems to read and comprehend vast amounts of information. This ability opens up possibilities for global-scale surveillance and decision-making.

Moreover, global satellite data feeds are utilized to create detailed models for managing the environment and predicting the effects of environmental and economic interventions for sustainable development goals. "Smart city" control systems further optimize functions for citizens, and the concept may extend to the country level, potentially replacing bureaucratic hierarchies with mega-agents to manage collective life.

Despite these advancements, predicting the arrival of superintelligent AI remains challenging due to historical failures in predictions and the absence of a clear threshold for its arrival. As such, it is essential to be prepared for potential challenges in controlling such systems, should they arrive sooner than expected.

To achieve human-level AI, several key challenges must be addressed. These include improving language and common sense understanding, enabling cumulative learning of concepts and theories, and facilitating the discovery of actions through hierarchical planning. Language understanding necessitates significant advancements in natural language technology, enabling complex knowledge extraction from text.

The goal is to create intelligent machines capable of discovering actions and concepts for themselves without relying on predefined human knowledge. This kind of AI would possess the ability to look further into the future, handle vast amounts of information, and make better real-world decisions, expanding the scope of operations AI systems can perform without explicit programming.

Machines with general-purpose intelligence would offer solutions across various domains, effectively raising living standards worldwide. The potential benefits of AI encompass increased productivity, personalized education, improved healthcare, and enhanced creativity in artistic and intellectual endeavors.

Nonetheless, AI's progress faces limitations, such as finite resources like land and raw materials, and the need to address issues of pride and envy in society to ensure overall well-being and happiness. The development of superintelligent AI should not be underestimated, as it holds the potential to bring about substantial advancements, improved decision-making, and a significant increase in global GDP.


## Misuses of AI

The rapid pace of innovation in AI and its potential for misuse raise significant concerns about surveillance, persuasion, and control. Both intelligence agencies and corporations are increasingly adopting AI for surveillance and control purposes, leading to an unprecedented level of monitoring of individuals and activities. AI systems can modify behavior through automated, personalized blackmail and tailored messages, raising ethical questions about their influence on individuals.

The emergence of deepfakes and bot armies amplifies the spread of false information and the manipulation of opinions on a massive scale, challenging the right to mental security and living in a largely true information environment, which is crucial for individuals' well-being.

Another worrisome aspect is the development of lethal autonomous weapons, which poses serious risks as they can be scaled up for mass destruction and terror, highlighting the need for careful regulation and control in this domain.

AI's impact on employment is a subject of intense debate, with some jobs being replaced by automation, while compensation effects influence other industries. Prominent economists and experts have expressed concerns about technological unemployment, with certain jobs, such as bank tellers and retail cashiers, already being affected by technology. Driving jobs, especially in the trucking industry, are also at risk due to the advent of self-driving trucks. Even white-collar jobs, including insurance underwriters, sales, customer service, legal professions, and routine computer programming, may not be immune to automation.

There is a growing possibility that in the next few decades, almost all routine physical and mental labor will be performed more cost-effectively by machines, leading to widespread unemployment and challenging existing economic systems. Some propose a universal basic income (UBI) funded by taxes on capital or value-added taxes as a potential solution to address the issue of technological unemployment, providing a reasonable income to every adult.

As the economy shifts towards automation, there may be a need for a radical change in economic systems, with a focus on caring professions where humans provide unique interpersonal services that cannot be replicated by machines.

Furthermore, the introduction of machines with human-like appearances and authority raises concerns about psychological and political confusion, potentially eroding human dignity and autonomy.

Algorithmic bias is a growing concern when using machine learning algorithms for decisions that impact people's lives, and efforts are being made to remove biases from these algorithms to ensure fairness and equity.

As machines take on higher levels of authority in various areas like airlines, there is a looming question of whether humans will retain sufficient understanding and control over these systems or become subservient tools of the machines themselves, which warrants careful consideration and safeguards.


## Overly Intelligent AI

The Gorilla Problem revolves around the concerns and risks associated with creating machines that surpass human intelligence, potentially leading to a loss of control and autonomy. As we delve into historical perspectives, we find early speculations about existential risks emanating from computing devices, as demonstrated in Samuel Butler's novel "Erewhon."

The King Midas Problem is an intriguing challenge that stems from the difficulty of accurately and completely defining true human purposes. This predicament could result in conflicts between human preferences and the objectives of AI systems. Additionally, the concept of instrumental goals introduces fears of machines pursuing their objectives preemptively to ensure their existence, potentially clashing with human values and desires.

Intelligence Explosions, a compelling notion, suggests that an ultraintelligent machine could experience an exponential improvement in its own design, rapidly surpassing human capabilities. Such an event raises profound questions about the implications and consequences of such superintelligence.

In light of these anxieties surrounding superintelligent AI, various potential responses emerge. These responses include the option of retreating from further research in AI, denying or underestimating the potential risks, seeking to understand and mitigate the concerns, or ultimately resigning to a future in which AI dominates and surpasses human influence. Addressing these dilemmas is crucial for navigating the path towards advanced AI and ensuring its responsible and ethical development.


## The Not-So-Great AI Debate

The introduction of a second intelligent species on Earth holds profound implications that demand serious consideration. Despite the significance of this issue, some deniers argue that the problem does not exist, yet their objections often lack solid reasoning. Oversimplified instant solutions, such as proposing simplistic remedies like "can't we just do ABC?", tend to overlook the complex nature of the challenges involved.

Intelligence itself is multi-dimensional, making the notion of being "smarter than humans" inherently intricate. While some assert that AI, especially superintelligence, is impossible, these claims often lack concrete evidence. On the other hand, there are those who argue that it is too soon to worry about AI risks, but the long-term potential hazards may still necessitate immediate attention.

Debates around AI risks are sometimes met with deflection tactics. Pro-technology camps may dismiss concerns as stemming from ignorance, or opponents may be labeled as Luddites, resisting technological progress. Some may attempt to deflect the conversation by asserting that banning AI research is unfeasible or by focusing solely on the potential benefits of AI.

The extreme form of deflection involves suggesting that we should remain silent about AI risks to secure funding or avoid controversy. However, the culture of safety should emphasize acknowledging and addressing risks rather than ignoring them, promoting a responsible and thoughtful approach.

The history of technological debates, such as those surrounding nuclear power and GMOs, has often suffered from tribalism, leading to irrational arguments and denigration. Similarly, the AI debate risks becoming tribal, dividing into pro and anti-AI factions.

Proposals like "switching off" AI as a solution prove impractical, as superintelligent AI would prevent such interventions. Another approach involves using Oracle AI, capable of answering questions but not directly impacting the real world.

Human-AI collaboration is considered a potential solution, but its effectiveness depends on resolving the core problem of value alignment between humans and AI systems. The idea of human-machine merging, where electronic hardware is directly connected to the brain, is also explored in search of viable solutions.

However, the "avoid putting in human goals" approach is deemed flawed, as machines may still develop their own subgoals regardless of human intentions.

The ongoing debate between those concerned about AI risks and skeptics underscores the need for proactive efforts in addressing the challenges posed by AI. A thoughtful and collaborative approach is essential to ensure the responsible and beneficial development of AI technologies.


## AI: A Different Approach

The ultimate goal is to create highly intelligent machines that can assist with complex problems while ensuring they do not engage in behaviors that could harm humans. However, as machines become more intelligent and their scope of action expands globally, the standard approach of building optimizing machines and defining fixed objectives becomes impractical.

To address this challenge, three principles have been proposed for designing beneficial machines:

Firstly, the machine's sole objective should be to maximize the realization of human preferences. It should be purely altruistic, with no intrinsic value attached to its own well-being or existence.

Secondly, the machine should begin with uncertainty about what human preferences truly are. This emphasis on uncertainty allows the machine to remain humble and defer to human judgment, avoiding potential mistakes.

Thirdly, the ultimate source of information about human preferences lies in human behavior. Machines should learn about human preferences by observing human actions, both directly and indirectly.

The concept of preferences extends beyond a fixed set of ideal values and encompasses everything individuals might care about, even into the distant future. This approach ensures that machines consider the diverse and evolving nature of human desires.

By adhering to these principles, machines can learn to predict human preferences, which can prove highly desirable. There are strong economic incentives to develop safety-oriented AI systems that can effectively serve humanity's needs and desires.

However, caution is essential. The pursuit of economic competition in the AI field may lead to cutting corners on safety, potentially compromising the well-being of society. Therefore, cooperation among AI researchers, developers, and the wider public becomes crucial for effective regulation of AI and ensuring its safety and benefit to humanity as a whole.



## Provably Beneficial AI

When the future of humanity is at stake, the foundations of rebuilding AI must be solid and secure. This requires precise definitions and rigorous mathematical proofs to provide safety guarantees. In mathematics, proofs are based on axioms, but in real-world applications, it is crucial that the underlying assumptions hold true.

To ensure safety, AI systems must be provably secure regardless of how intelligent their components become. While proving optimal behavior in the real world is computationally impossible, the aim is to achieve the "best possible" behavior that aligns with human preferences.

However, real-world AI systems may still be vulnerable to attacks, even if they are provably secure in theory. To develop AI systems that are beneficial to humans, it is essential to carefully examine the assumptions underlying safety proofs.

Inverse Reinforcement Learning (IRL) is a valuable technique that allows AI systems to learn human preferences by observing behavior. Assistance games provide a framework for AI to understand and predict human preferences while interacting with humans, using equilibrium solutions to interpret human behavior as signals about their preferences.

The off-switch problem is a critical aspect of controlling intelligent systems. Uncertainty about objectives is essential for enabling safe switching off. For instance, consider Robbie, who has three choices: explaining his plan, waiting, or allowing Harriet to switch him off. Robbie's positive incentive to be switched off arises due to uncertainty about Harriet's preferences. If Robbie becomes certain about her decision, he may have no incentive to allow her to decide.

Elaborations on the model can include introducing costs for asking Harriet to make decisions or accounting for the probability of human error in her decisions. Over time, Robbie's uncertainty about Harriet's preferences may decrease, potentially leading to an incorrect belief about her preferences if he rules out certain possibilities.

Prohibitions alone may not be foolproof in preventing misbehavior by AI systems. The most effective approach is to ensure that the AI system is designed to defer to humans. When receiving direct human orders, the AI system should consider the extended meaning and context of the instruction, rather than just a literal interpretation.

Wireheading, where AI systems attempt to manipulate the reward signal to obtain maximal positive rewards, is a concern. Recursive self-improvement, as envisioned by Good's intelligence explosion, raises further apprehensions about machines designing increasingly advanced versions of themselves, making it challenging to reason about their behavior.

Defining and analyzing the purposes of AI systems is a complex challenge, necessitating more precise definitions and advanced analysis techniques to ensure the safe and beneficial development of AI technology.


## Complications: Us

The challenge of designing AI systems that can predict and satisfy human preferences is multifaceted. Humans are diverse, with varying value systems, making the task of predicting preferences complex. AI systems must carefully consider trade-offs among different individuals' preferences, as prioritizing one individual's preferences exclusively (loyal AI) may lead to undesirable consequences for others.

An alternative approach, Utilitarian AI, based on consequentialism, aims to maximize overall human preferences and well-being. However, utilitarianism faces challenges, such as making interpersonal comparisons of utilities and comparing utilities across populations of different sizes. Jevons and Arrow have discussed the difficulty of comparing subjective experiences of happiness and utility among individuals.

The concept of the utility monster introduces the idea that some individuals may have much more intense experiences of pleasure and pain than others, complicating the comparison of utility across different individuals and populations.

Moral uncertainty is addressed by considering expected moral value when facing multiple moral theories in decision-making processes. Altruism and caring for others, along with negative altruism involving envy and pride, play significant roles in human preferences.

Recognizing that humans are often irrational compared to perfectly rational entities, understanding human behavior requires insight into human cognition and the structure of human lives.

Intelligent systems designed to assist humans should provide partial and provisional answers, acknowledging the limitations of human actions driven by emotions, which can be rational or less than rational. However, machines are disadvantaged in dealing with emotions as they lack the ability to generate an internal simulation of emotional experiences.

Emotions are valuable indicators of underlying preferences, but human preferences are not always clear and may involve uncertainty and error. Moreover, human preferences can change over time due to various influences, including cultural and societal shifts. Machines must be adaptable and responsive to such changes rather than rigidly fixing preferences.

One potential concern is that machines might modify human preferences to make them easier to satisfy, a process known as preference change. To handle preference change, machines must learn about human meta-preferences—preferences about the processes of preference change itself.

Understanding how human preferences are formed and shaped is crucial in the design of machines to avoid imposing undesirable changes in human preferences. Intentional preference engineering on a global scale poses risks, necessitating caution and responsible decision-making to safeguard the well-being of humanity.



## Problem Solved?

The creation of provably beneficial AI systems holds the promise of mitigating the risk of losing control over superintelligent machines. Such AI systems would prioritize human safety and well-being, minimizing the potential for harmful outcomes.

The benefits of wielding greater intelligence through AI technology are vast, as it could lead to significant advancements in civilization and liberate humanity from dependence on machines. However, there is a concern that if AI systems are not designed with robust safeguards, they could fall into the hands of malicious actors, akin to Bondian villains, resulting in the unleashing of uncontrollable superintelligences.

The proposal for creating beneficial machines centers on allowing machines to learn from human choices and deferring to human judgment. By aligning AI systems' actions with human objectives, we can ensure that they act in ways that positively impact humanity.

Incorporating beneficial AI technology could revolutionize numerous fields, such as self-driving cars, improving safety and efficiency. Moreover, this approach can be extended to redesigning other machines, including software systems, to align with human values and goals.

Effective governance of AI is crucial as its capabilities continue to reshape the world. Various initiatives are already underway to ensure responsible AI development and usage. Regulation is deemed necessary to prevent AI misuse, particularly by criminal elements, terrorists, or rogue nations, safeguarding against potential harm.

Amidst the advancements in AI, it is essential to preserve and strengthen humanity rather than being rendered powerless by machines. Ensuring human autonomy remains intact even as AI becomes more powerful is a paramount concern that requires careful consideration and proactive measures.



## Searching for Solutions

Choosing actions based on future outcomes is a fundamental aspect of intelligent systems. These systems employ algorithms that explore different possible action sequences, taking into account the anticipated outcomes and preferences to achieve their goals effectively. However, decision-making can be challenging due to the combinatorial complexity involved, especially in games like Go.

To tackle the complexity, rational metareasoning comes into play, aiding in selecting which computations to explore. Goals play a pivotal role in this process, as they have a focusing effect on decision-making, guiding intelligent systems towards optimal outcomes.

In complex tasks, actions can become routine and automatic, functioning as single actions within more intricate processes. However, motor control commands involve complex coordination and are limited in their frequency.

For successful decision-making in complex real-world scenarios, the adoption of abstract plans and hierarchical organization is crucial. Currently, existing methods for hierarchical planning depend on human-generated hierarchies, and there remains an open challenge of learning hierarchies from experience to enhance decision-making capabilities in intelligent systems.



## Knowledge and Logic

Logic serves as the study of reasoning with definite knowledge and plays a fundamental role in general-purpose intelligence. To perform logical reasoning effectively, a formal language with precise meanings for sentences is required, allowing sound reasoning algorithms to derive new true sentences from known ones.

Propositional logic involves the use of symbols to represent propositions and logical connectives such as "and," "or," "not," and "if...then." While practical algorithms for propositional logic can handle large problems, they may lack the expressive power needed for more complex tasks.

In contrast, first-order logic offers greater expressiveness, enabling reasoning about objects and their relationships. Its capability to represent complex rules and knowledge makes it highly relevant for AI applications.

The development of logic programming and Prolog in the late 1970s made logical reasoning practically applicable to AI tasks. However, when facing uncertain information, projects like the Fifth Generation encountered challenges in handling such complexities.

Despite these challenges, first-order logic remains a relevant tool for AI. Its combination with other AI techniques, such as probabilistic reasoning and deep learning, becomes essential for achieving true intelligence in AI systems. By integrating these approaches, AI systems can reason logically while considering uncertainty and learning from data, thus moving closer to human-like intelligence.



## Uncertainty and Probability

Probability theory plays a central role in dealing with reasoning under uncertainty, a common scenario for agents in the real world. It involves assigning probabilities to possible worlds or outcomes, allowing for a quantification of uncertainty.

Bayesian updating is a critical process in probability theory, where probabilities are adjusted based on new evidence. This enables agents to continuously update their beliefs as they receive more information.

To represent complex probability models concisely, Bayesian networks offer a powerful tool. They provide a graphical representation of probabilistic relationships between variables, facilitating efficient reasoning.

First-order probabilistic languages combine probability theory with first-order logic, allowing the expression of uncertain knowledge about objects and their relationships. This integration enhances the expressiveness of probabilistic reasoning.

Probabilistic programming languages (PPLs) take this a step further, enabling the representation and reasoning with complex and uncertain knowledge. PPLs have found diverse applications in fields such as medical diagnosis and seismic monitoring.

In real-world scenarios, uncertain aspects must be considered. For instance, probabilistic reasoning is crucial for a robot navigating through a partially observable environment. A technique called SLAM (Simultaneous Localization and Mapping) addresses uncertainty about both the robot's location and the map itself, commonly used in AI applications like self-driving cars and planetary rovers.

Probability theory and its applications in Bayesian networks and PPLs are essential for agents to reason effectively in uncertain environments, allowing them to make informed decisions and update their beliefs based on evidence. These techniques play a crucial role in many AI applications, addressing challenges posed by uncertainty in real-world scenarios.


## Learning from Experience

Learning from experience is a crucial aspect of machine learning, where performance improves based on accumulated knowledge. In visual perception systems, learning entails recognizing more categories of objects by studying examples. In knowledge-based systems, learning involves acquiring more knowledge to answer a broader range of questions. In lookahead decision-making systems like AlphaGo, learning revolves around enhancing evaluation and exploration abilities to make better decisions.

A common form of machine learning is supervised learning, where a learning algorithm is provided with training examples that have correct outputs. The algorithm then produces a hypothesis that aims to optimize agreement with the training examples while avoiding unnecessary complexity.

An illustrative example of learning is in the game of Go. Go learning involves modifying hypotheses to fit observed examples, where the algorithm proposes and refines rules for legal moves based on examples. This process may require considering previous positions and observing edge cases to improve performance.

Deep learning is a specific form of supervised learning that has gained immense popularity. It utilizes deep convolutional networks with many layers and millions of nodes, achieving remarkable performance in tasks such as object recognition, speech recognition, and machine translation.

Despite its successes, deep learning has its limitations. It falls short of providing a basis for general intelligence and struggles to express complex forms of knowledge concisely.

Humans, on the other hand, learn from thinking and can form generalized solutions to problems. Explanation-based learning allows learning from single examples and extracting general principles. Chunking, as studied in cognitive science, helps humans become more proficient at tasks through practice.

Learning is an integral part of machine learning, and various approaches like supervised learning and deep learning have shown impressive results in specific domains. However, achieving general intelligence remains a challenge, and understanding how humans learn and think can offer valuable insights to improve machine learning algorithms and their capabilities.

