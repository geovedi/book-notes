# Code
Charles Petzold

***

"Code" by Charles Petzold is a captivating exploration of the history, principles, and significance of coding, taking readers on a journey from the early days of computing to the graphical revolution that transformed how we interact with computers.

The book begins by delving into the origins of coding, starting with the invention of Morse code and progressing through the development of telegraphy and early computing devices. Petzold discusses the pivotal role of electricity in enabling communication and how it paved the way for the digital age.

Moving on, the book introduces readers to the fundamental components of electrical circuits and how they form the building blocks of modern computing. From basic switches to logic gates, readers gain insight into how these components work together to perform complex calculations and data processing.

As the narrative unfolds, the focus shifts to the evolution of programming languages. Petzold explores the emergence of high-level languages and their impact on computer programming. He discusses early languages like ALGOL and COBOL, as well as influential languages like C, which played a significant role in shaping modern programming paradigms.

A significant portion of the book is dedicated to the graphical revolution, which transformed computing through the development of graphical user interfaces (GUIs). Petzold highlights the pioneers of graphical interfaces, such as Xerox PARC, and their contributions to the introduction of windows, icons, menus, and pointers (WIMP) – elements that revolutionized how users interact with computers.

The book also covers the development of computer graphics, from the use of bitmap displays to the integration of sound and video in multimedia applications. Petzold explores the technical aspects of graphics and how they enabled advancements in various fields, such as video games, CAD programs, and animation.

Throughout "Code," Charles Petzold emphasizes the critical role of code in shaping the modern world. He discusses the impact of code on industries, communication, and everyday life, highlighting its significance in everything from the simplest electrical circuits to complex computer systems.

"Code" offers readers a comprehensive and engaging look into the history and principles of coding, providing a deeper understanding of how computers work and their profound influence on society. The book serves as an insightful and accessible guide for both tech enthusiasts and general readers interested in the fascinating world of coding and its transformative power.

***

## Best Friends
You're 10 years old, living across the street from your best friend, and your bedrooms' windows face each other. Every night, after your parents declare bedtime at an unjustly early hour, you feel the need to exchange thoughts, secrets, and dreams with your best friend. While the lights are on, you can wave to each other and use rudimentary body language to convey simple thoughts. But once the lights go out, you find yourselves faced with a challenge.

How can you communicate effectively in the dark? The telephone is not a viable option, as it might be overheard. Although your family's computer is connected to a phone line, it's not conveniently located in your room. However, you both have flashlights. A brilliant idea strikes you – could you make flashlights speak?

You experiment with a method where each letter of the alphabet corresponds to a specific number of flashlight blinks. For instance, an "A" is one blink, "B" is two blinks, and so on up to 26 blinks for "Z." It seems promising, but you encounter a problem – the messages become too lengthy and difficult to decipher.

Recalling scenes from movies, you explore other alternatives like signaling with blinking lights. This leads you to discover Morse code – a more efficient and widely-used system of dots and dashes representing letters of the alphabet. Morse code provides a concise way of communicating, requiring fewer blinks to convey a message.

You start learning Morse code and realize that each letter corresponds to a unique combination of dots and dashes. It becomes apparent that this code is more structured than your initial invention. The length of the pauses between dots and dashes also matters, making the communication clearer and less prone to confusion.

The excerpt delves into the nature of codes, emphasizing their significance in human communication. From spoken language to various written codes like sign language, Braille, and shorthand, different codes serve different purposes, each designed to meet specific communication needs.

Though Morse code isn't directly related to computers, understanding it lays the groundwork for comprehending the hidden languages and structures of computer hardware and software. The excerpt highlights how codes are an integral part of human communication and play a vital role in the world of technology.

As you and your best friend practice sending and receiving Morse code messages with your flashlights, you discover the joy of a new language – one that reduces written language to dots and dashes, and spoken language to just two vowel sounds. This newfound fascination with codes hints at the captivating journey you are about to embark on in the realm of computers and information technology.

## Codes and Combinations
Morse code, the ingenious system of dots and dashes used for communication, was the brainchild of Samuel Finley Breese Morse (1791–1872), a name that we'll delve into further in this book. The invention of Morse code is intimately tied to the creation of the telegraph, a topic we will explore in more depth as well. Just as Morse code offers insight into the nature of codes, the telegraph provides a gateway to understanding the hardware of computers.

Learning Morse code is often easier when sending messages rather than receiving them. When receiving code and translating it back into words, the process becomes more challenging and time-consuming. Working backward to deduce the letter represented by a specific sequence of dots and dashes requires careful scrutiny of the translation table.

While the table presents the alphabetical letters and their corresponding Morse code sequences, there isn't an immediate reverse table that lets us go from Morse code back to alphabetical letters. However, the excerpt explores an alternative way to organize the codes – grouping them based on the number of dots and dashes they contain.

Organizing the codes into four tables based on the number of dots and dashes – 1, 2, 3, and 4 – reveals a clear pattern. Each table contains twice as many codes as the one before it. This pattern can be expressed succinctly using powers of 2: the number of codes is 2 raised to the power of the number of dots and dashes.

The excerpt also introduces the concept of a "big treelike table," which visually displays the letters corresponding to specific sequences of dots and dashes. Following the arrows from left to right helps in decoding the Morse code.

Expanding the system to include five dots and dashes accounts for numbers and punctuation marks, and using six dots and dashes extends the possibilities further. The binary nature of Morse code becomes evident as it employs only two components – dots and dashes – similar to a coin landing on heads or tails. Combinatorial analysis, a branch of mathematics, helps in understanding the various ways codes can be combined and deconstructed.

Morse code provides a fascinating glimpse into the world of codes and their patterns. It serves as a foundation for grasping the complexities of communication systems, both past and present, and offers essential insights into the building blocks of computer technology.

## Braille and Binary Codes
While Samuel Morse is renowned for developing the Morse code, he wasn't the first to create a code for written language. That distinction belongs to Louis Braille, a brilliant blind French teenager born about 18 years after Morse. Despite his blindness, Braille's thirst for knowledge and intelligence were evident from a young age.

At three years old, a tragic accident left Braille blind when a pointed tool pierced his eye, leading to an infection that spread to the other eye. Despite the bleak prospects for blind individuals during that time, Braille's potential was recognized by a village priest and a schoolteacher, which ultimately led to his education at the Royal Institution for Blind Youth in Paris.

The education of the blind faced a significant hurdle – their inability to read printed books. Valentin Haüy had earlier developed a system of raised letters on paper, but it proved challenging to use and limited in availability. The breakthrough came from an unexpected source, Charles Barbier, a French army captain, who devised "night writing" using raised dots and dashes on paper. While his system had flaws, it provided a foundation for Braille's future work.

Louis Braille encountered Barbier's system at the age of 12 and was drawn to the simplicity and ease of reading and writing using raised dots. Over the next three years, Braille diligently refined the system, and at the age of 15, he had devised the basis of what we know as Braille today. Initially, the system was limited to the school, but its reach expanded over time.

Today, Braille remains a vital system for providing access to the written word for the blind, especially for those who are also deaf. While modern technology has introduced enhanced versions of Braille, it remains an essential means of communication for the visually impaired.

In this chapter, we will explore the intricacies of the Braille code. Every symbol used in written language, including letters, numbers, and punctuation marks, is encoded through raised dots arranged within a two-by-three cell. Each dot in the cell can be either flat or raised, reflecting a binary nature similar to Morse code.

With six dots, there are 64 possible combinations in Braille, but not all are utilized. Louis Braille's original system accounted for 64 codes, each representing a unique character. The system used today, Grade 2 Braille, employs contractions to save space and speed up reading.

Braille's ingenuity can be seen in the organization of the code. For example, the first row uses the top four dots (dots 1, 2, 4, and 5), the second row duplicates the first row and adds dot 3, and the third row duplicates the second row and adds dot 6.

Braille's system accommodates letters, numbers, punctuation marks, and even capitalization indicators. It cleverly uses shift codes and escape codes to change the interpretation of subsequent codes, allowing for a more efficient representation of written language.

Through binary elements, Braille brings accessibility and empowerment to the blind community, ensuring that they can partake in the joys of literature and information just like anyone else. The Braille code stands as a testament to the brilliance of Louis Braille, forever changing the way the blind read and write.

## Anatomy of a Flashlight
Flashlights are versatile tools, serving numerous purposes beyond the obvious tasks of reading under the covers or sending coded messages. They can also take center stage in educational demonstrations about the mysterious yet pervasive phenomenon known as electricity. Although electricity remains a source of wonder for many, it is essential to grasp a few basic concepts to understand its applications in computers.

At its core, a flashlight is one of the simplest electrical appliances found in most homes. Disassembling a typical flashlight reveals a straightforward construction consisting of a couple of batteries, a bulb, a switch, some metal pieces, and a plastic case to hold everything together. For a no-frills flashlight, one can eliminate everything except the batteries and the lightbulb, and use short pieces of insulated wire to create a simple electrical circuit with a switch.

The fundamental idea behind an electrical circuit is that it forms a circular path. The lightbulb will only glow if the path from the batteries to the wire, bulb, switch, and back to the batteries remains uninterrupted. Any break in this circuit will cause the bulb to go out, and the purpose of the switch is to control this process.

Electricity itself is closely related to the movement of electrons. Every atom, the basic building block of matter, contains protons, neutrons, and electrons. When certain circumstances arise, electrons can be dislodged from atoms, leading to the occurrence of electricity.

Batteries play a crucial role in generating electricity. Chemical reactions inside the batteries result in an excess of electrons at the negative terminal and a demand for electrons at the positive terminal. When an electrical circuit is connected, the electrons flow through the wires, creating a current. Substances like copper, silver, and gold are excellent conductors, allowing electrons to move easily, while materials such as rubber or plastic resist the flow of electrons and are considered insulators.

Voltage, measured in volts, represents the potential for doing work, much like the potential energy of an elevated brick. On the other hand, current, measured in amperes or amps, signifies the actual flow of electrons in a circuit. Resistance, measured in ohms, determines how much a substance impedes the flow of electrons.

By connecting two batteries in series, their voltage adds up to produce a higher potential difference. In contrast, connecting them in parallel preserves the voltage but increases the current capacity. An incandescent lightbulb, like the one used in a flashlight, relies on its resistance to generate heat and light.

Switches control the flow of electricity in a circuit. When the switch is closed, allowing electricity to flow, it is said to be on, while an open switch prevents the flow of electricity. The simplicity of a flashlight's binary nature—either on or off—resembles the binary codes invented by Morse and Braille, and this similarity proves advantageous in understanding electrical circuits and their applications.

## Seeing Around Corners
You're twelve years old, and after your best friend moves away, you're determined to keep the late-night flashlight Morse code conversations alive with your new best friend next door. But there's a problem—your bedroom windows don't face each other. Undeterred, you decide to build your own flashlights using batteries, lightbulbs, switches, and wires. With some creativity, you manage to create a two-way telegraph system that allows you to send and receive messages using Morse code.

By using a common connection, you reduce the number of wires needed to transmit messages between your houses. The common, which is essentially the earth or ground, acts as a massive conductor that facilitates the flow of electrons between your two circuits. With this setup, you can communicate around corners and over long distances just by extending the wires.

However, you realize that as the wires get longer, the resistance increases, leading to dimmer lightbulbs and weaker signals. To combat this, you experiment with thicker wires and higher-voltage batteries to maintain a reliable communication system. Yet, you still face limitations, much like the early telegraph systems that spanned only a few hundred miles.

But fret not—there's a simple and humble device on the horizon that will revolutionize communication and pave the way for the creation of entire computers.

## Telegraphs and Relays
In the early 1800s, instantaneous worldwide communication was merely a dream. Long-distance communication relied on slow and unreliable methods like semaphore codes or waving flags on hills. However, Samuel Finley Breese Morse, born in 1791, was about to change the course of history.

Morse was an eclectic individual, a successful portrait artist, and even ran for mayor of New York City. But his most enduring legacy was the invention of the telegraph and the code that bears his name. The idea of the telegraph, or "far writing," had been explored by other inventors, but Morse's brilliance lay in his use of electromagnetism.

The telegraph system used an electromagnet that could attract pieces of iron when a current passed through a wire wrapped around it. By devising a code using dots and dashes (Morse code), messages could be sent and received over long distances. The historic day of the first public demonstration was on May 24, 1844, when the message "What hath God wrought!" traveled from Washington, D.C., to Baltimore, Maryland.

The telegraph key was the means of sending messages, producing dots and dashes based on the duration the key was held down. At the receiving end, the sounder translated these dots and dashes into audible clicks and clacks. As telegraph lines extended across the country, a new challenge arose: the resistance of long wires limited the system's reach.

Enter the relay, a remarkable device that Morse had foreseen early on. The relay used an incoming current to activate an electromagnet, which in turn, controlled a switch to amplify the outgoing current. This ingenious invention allowed weak incoming signals to be strengthened for longer transmission distances.

The relay's potential didn't go unnoticed. Its ability to amplify electrical signals sparked curiosity about what else could be achieved with such devices. Little did they know that this relay was the first step towards building something much greater – a computer. But before diving into that territory, there was one essential skill to learn: counting.

## Our Ten Digits
Language is like a code, and we can grasp the idea that the same object can be called by different names in different languages. However, numbers seem more universally consistent. Regardless of the language we speak, the written representation of numbers remains largely unchanged and easily understood worldwide.

Numbers are the epitome of abstraction. When we see the symbol "3," it doesn't necessarily bring to mind a specific object; instead, it represents a quantity that can apply to various contexts. The abstract nature of numbers makes it challenging to comprehend that the same quantity can be denoted by different symbols, like "3," "III," or "三."

Despite the widespread use of a base-ten number system, there's nothing inherently special about the number ten. This choice is merely a result of early humans counting on their fingers, and had they had a different number of fingers, our system might be different. Still, we assign special significance to powers of ten, like a decade, a century, or a millennium.

The concept of numbers began with the need to count objects, leading to the development of various number systems, such as Roman numerals. Roman numerals are still in use today, albeit mostly for decorative purposes. While adding and subtracting Roman numerals is straightforward, multiplication and division become cumbersome.

The Hindu-Arabic number system, brought to Europe by Arab mathematicians, revolutionized mathematics with its positional nature. The position of a digit within a number carries significance, making it easy to represent large numbers and perform operations like addition and multiplication. The Hindu-Arabic system's most crucial component is the zero, a seemingly insignificant invention that profoundly impacted mathematics.

Using the Hindu-Arabic system, we can break down numbers into components, expressing them as sums of powers of ten. This versatility extends to decimal numbers as well, making arithmetic operations systematic and manageable. The convenience of this positional notation lies in breaking complex problems into simple steps of addition and multiplication with single-digit numbers.

The beauty of the Hindu-Arabic system lies not only in its effectiveness but also in its adaptability. For cartoon characters, who often have four fingers, a base-eight numbering system may be more suitable. Despite our preference for base ten, the concepts of positional notation and place value apply universally, allowing us to appreciate and understand the significance of numbers across different cultures and contexts.

## Alternatives to Ten
Ten is a significant number for humans due to the number of fingers and toes we possess. This fact led us to develop the base ten, or decimal, number system. However, alternative number systems can exist if we had a different number of fingers. For instance, if humans had only four fingers on each hand, we might have developed a base eight system called octal.

In the octal system, we count from 0 to 7 before running out of symbols, just like we do in the decimal system from 0 to 9. The octal number 10 doesn't represent ten fingers; it represents the number of fingers cartoon characters have in their hands.

The octal system is structured in the same way as the decimal system, with each position in a number representing a digit multiplied by a power of eight. For example, the octal number 3725 can be broken down into 3000 + 700 + 20 + 5.

Moving further, we encounter the quaternary system (base four), which would be suitable for creatures like lobsters that have pincers on their two front legs. Counting in quaternary goes from 0 to 3 before running out of symbols.

The binary system (base two) is the most extreme alternative. In this system, there are only two digits: 0 and 1. The rightmost digit alternates between 0 and 1, and the subsequent digits change with each transition from 1 to 0 in the rightmost digit. Binary numbers can be very long but are essential in understanding computers since they operate with electrical switches that can represent binary digits (0 and 1).

The mathematician John Wilder Tukey introduced the term "bit" to refer to a binary digit, realizing its importance as computers became more prevalent.

Different number systems like octal, quaternary, and binary provide us with valuable insights into the fundamentals of arithmetic and its connection to computer technology.

## Bit by Bit by Bit
Bits are the building blocks of information in the digital realm, representing a simple choice between 0 and 1. We draw parallels with the use of a yellow ribbon to convey a clear yes or no message and apply this concept to binary numbers, the simplest number system possible.

Binary numbers form the basis of computer systems, making them incredibly powerful yet easy to work with. We explore how bits can represent choices and discuss the value of redundancy in communication, using examples from Paul Revere's famous lantern signals and Siskel and Ebert's movie reviews.

Bits are everywhere, hidden in the technology we use daily, from cameras and barcodes to Braille. We learn how Universal Product Codes (UPCs) encode information with bits and include built-in error-checking mechanisms. While decoding UPCs may seem complex, we find that the information they convey is often quite mundane—much like the numbers printed on the package. However, we come to appreciate the importance of redundancy and error-checking in these systems.

## Logic and Switches
What is truth? Aristotle thought that logic had something to do with it. The collection of his teachings known as the Organon (which dates from the fourth century B.C.E.) is the earliest extensive writing on the subject of logic. To the ancient Greeks, logic was a means of analyzing language in the search for truth and thus was considered a form of philosophy.

The basis of Aristotle's logic was the syllogism. The most famous syllogism is "All men are mortal; Socrates is a man; Hence, Socrates is mortal." In a syllogism, two premises are assumed to be correct, and from these, a conclusion is deduced.

For over two thousand years, mathematicians wrestled with Aristotle's logic, attempting to corral it using mathematical symbols and operators. Prior to the nineteenth century, the only person to come close was Gottfried Wilhelm von Leibniz (1648–1716). And then came George Boole.

George Boole, born in England in 1815, made a breakthrough in mathematical logic. He invented a kind of algebra, now known as Boolean algebra, which looks and acts much like conventional algebra. However, in Boolean algebra, the operands refer not to numbers but instead to classes. Boole's algebra paved the way for representing logic and mathematical concepts in terms of switches and electrical circuits.

Boole's algebra introduced the concepts of union and intersection, which are represented by + (OR) and x (AND) symbols, respectively. For instance, the expression "M x N x (W + T)" represents a neutered male cat that is either white or tan. Boole's work eventually led to the development of computers that work with binary numbers, using switches and circuits to perform logical operations.

Though Boole's genius was not fully appreciated in his time, his work laid the foundation for the digital age and the modern understanding of logic and computing.

## Gates (Not Bill)
In the history of primitive computing, the term "logic gates" might mistakenly be attributed to Bill Gates, the co-founder of Microsoft. However, this is far from the truth. Logic gates share a resemblance with ordinary gates that control the flow of water or people. In the realm of logic, these gates perform simple tasks by allowing or blocking the flow of electrical current.

To understand their significance, let's recall the previous chapter's example of choosing a specific cat in a pet shop using Boolean expressions and a circuit of switches and a lightbulb. In the past, nobody realized that these electrical circuits could directly represent Boolean expressions until the 1930s, when Claude Elwood Shannon made a breakthrough. He demonstrated that electrical engineers could utilize Boolean algebra to design circuits with switches efficiently.

In our cat example, the expression (M x N x (W + T)) + (F x N x (1 – W)) + B can be simplified to (N x ((M x (W + T)) + (F x (1 – W)))) + B using the associative and distributive laws. The simplified circuit represents the same cat selection, but with one less switch.

Ideally, we should be able to design a circuit with just four switches, as each switch corresponds to a bit (binary digit). One switch represents the sex (off for male, on for female), the second switch represents neutered status (on for neutered, off for unneutered), and the last two switches represent the cat's color. Four colors can be represented using 2 bits, enabling us to select the desired cat through combinations of switch positions.

By combining relays, electrical engineers create logic gates, which can perform more complex tasks. The four basic gates are the AND, OR, NOT (inverter), and NAND gates. These gates can be connected to build more sophisticated circuits, like the 2-Line-to-4-Line Decoder.

The NOR and NAND gates are equivalent to AND gates with inverted inputs and OR gates with inverted inputs, respectively. De Morgan's Laws state that the output of an AND gate with inverted inputs is equivalent to the output of a NOR gate with non-inverted inputs. Similarly, the output of an OR gate with inverted inputs is equivalent to the output of a NAND gate with non-inverted inputs.

With these tools, electrical engineers can simplify Boolean expressions and circuits, making it easier to design and implement complex systems, such as adding machines.

## A Binary Adding Machine
Understanding addition is paramount since almost all computer operations boil down to this basic arithmetic function. By creating an adding machine, we pave the way for more complex operations such as subtraction, multiplication, division, and various sophisticated computations that empower modern technology.

The adding machine we construct is not a high-tech marvel but a relatively simple device using electrical components that were invented over a century ago. We delve into the world of binary numbers, which will be the foundation for our adding machine. While our device may be slower, clunkier, and less sophisticated than today's calculators and computers, its inner workings are enlightening. Throughout this chapter, we explore the beauty of binary logic, utilizing switches, lightbulbs, wires, a battery, and prewired relays for various logic gates.

Our adding machine will employ binary numbers exclusively, which means inputting numbers via a row of switches and displaying results using a row of lightbulbs. Adding binary numbers is akin to adding decimal numbers but with a much simpler table to work with. The binary addition table consists of just four operations: 0+0=0, 0+1=1, 1+0=1, and 1+1=0 (with a carry of 1).

To achieve binary addition, we need to design a circuit that performs sum and carry operations separately. This leads us to the XOR (Exclusive OR) gate, which computes the sum bit, and the AND gate, which calculates the carry bit. The XOR gate acts like an addition without carrying, while the AND gate handles the carry bit. By combining these two gates, we create a Half Adder, capable of adding two binary digits. However, since most binary numbers are longer than one bit, we introduce the Full Adder to incorporate the carry bit from previous additions.

The Full Adder is the building block of our 8-bit adding machine. We wire eight Full Adders together, allowing us to add binary numbers up to 8 bits long, or 0 to 255 in decimal. Our control panel contains two rows of eight switches, representing the two 8-bit numbers to add, and a row of nine lightbulbs to display the result, accounting for the possibility of a 9-bit sum.

We progress from understanding basic binary addition to building the complete 8-Bit Adder, comprising 144 relays. While our machine still uses relays for educational purposes, modern computers use transistors to perform the same logic functions at a much faster and smaller scale.

## But What About Subtraction?
In the realm of binary arithmetic, after mastering the art of adding binary numbers using relays, one cannot help but wonder about subtraction. This chapter delves into the intriguing world of binary subtraction and how it differs from addition. While addition progresses steadily from the rightmost to the leftmost column, carrying over digits, subtraction requires a different mechanism—borrowing.

To understand subtraction without borrowing, we use a clever trick involving the nines' complement. To subtract one number from another without borrowing, we first calculate the nines' complement of the subtrahend. Then, we add it to the minuend, and finally, we add 1 and subtract 1000 to arrive at the correct result.

This technique works because when we add and subtract the same number in an expression, the result remains unchanged. The nines' complement allows us to avoid borrowing altogether, simplifying the process of binary subtraction.

When the subtrahend is larger than the minuend, we switch the two numbers, perform the subtraction using the nines' complement, and then negate the result to get the correct answer. This approach eliminates the need for borrowing while efficiently handling negative numbers.

The concept of ten's complement is also explored, where we represent negative numbers without using negative signs by defining a range of numbers that covers both positive and negative values. This approach simplifies addition, as we only need to follow the rules of addition, regardless of the sign.

In the realm of binary numbers, the two's complement system is analogous to ten's complement for decimal numbers. The most significant bit acts as the sign bit, with 1 representing negative numbers and 0 representing positive numbers. Two's complement allows us to freely add positive and negative numbers using only the rules of addition.

However, we must be cautious of overflow and underflow conditions, which occur when the result of an addition exceeds the allowed range of values. These conditions can lead to incorrect results, and proper handling is necessary to ensure accurate computations.

## Feedback and Flip-Flops
Electricity plays a significant role in powering various devices in our homes, from clocks and fans to food processors and CD players. It also contributes to generating sounds and music through loudspeakers in our stereo systems and televisions. Among the fascinating devices powered by electricity are the electric buzzers and bells, which are becoming rarer as electronic alternatives replace them.

Relays are an essential component in electrical circuits. A relay typically has an electromagnet that controls the position of a flexible contact. When the relay is energized, the contact changes position, completing or breaking a circuit. By continuously closing and opening the circuit, relays can create buzzing or clicking sounds, which can be used to build electric buzzers or bells.

An oscillator is a unique circuit that automatically generates a repetitive output without human intervention. It is different from other circuits that require manual switching to change states. The output of an oscillator rapidly alternates between 0 and 1, creating a clock-like signal. Frequency, measured in hertz (Hz), determines how fast the oscillator runs.

Flip-flops are circuits capable of storing information, effectively "remembering" previous states. One type of flip-flop is the R-S (Reset-Set) flip-flop, which has two inputs (S and R) and two outputs (Q and Q-bar). It can have two stable states when both inputs are open, making it ideal for memory applications.

The D-type flip-flop is another essential component that can "latch" onto a bit of data and retain it until a specific signal (Clock) triggers an update. It saves the input data when the Clock signal transitions from 0 to 1. This flip-flop is useful for storing intermediate values in circuits and facilitating counting operations.

By connecting an oscillator to a flip-flop as feedback, we can create a frequency divider. The output of this circuit alternates between 0 and 1 at half the frequency of the oscillator, effectively dividing the frequency. This counter can then be expanded into an 8-bit counter, counting in binary numbers from 0 to 255.

As we enhance flip-flops with features like preset and clear inputs, they become more sophisticated and versatile. These additional inputs allow us to set the output of the flip-flop directly, overriding the usual data input.

## Bytes and Hex
The word "byte" originated at IBM, likely around 1956, and it had its roots in the word "bite" but was spelled with a "y" to avoid confusion with "bit." At first, a byte simply referred to the number of bits in a specific data path. However, by the mid-1960s, during the development of IBM's System/360, the term evolved to signify a group of 8 bits.

Why 8 bits, though? There was no particular reason why it had to be 8 bits, but it seemed like a convenient amount—a nice "biteful" of bits. The improved adding machines in the previous chapter were based on the original one, which worked with 8-bit values, and thus the trend continued.

As an 8-bit quantity, a byte can represent values from 00000000 to 11111111. These values can correspond to positive integers from 0 to 255 or, if using two's complements to represent negative numbers, they can represent both positive and negative integers in the range from -128 to 127. Alternatively, a byte can represent one of 256 different things.

The byte size of 8 bits turned out to be ideal for various purposes. IBM found it convenient because it made storing numbers in the Binary-Coded Decimal (BCD) format (covered in Chapter 23) easier. Furthermore, most written languages around the world (excluding Chinese, Japanese, and Korean, which use ideographs) can be represented with fewer than 256 characters, making a byte suitable for storing text. Additionally, the human eye can distinguish approximately 256 shades of gray, making 1 byte perfect for representing gray shades in black-and-white photographs.

Though a byte is widely used in computer internals, expressing its values in binary can be cumbersome. To simplify this, we often use hexadecimal notation. In the hexadecimal (base-16) number system, each digit corresponds to 4 bits or 1 nibble, making it more concise when representing bytes. The digits in hexadecimal range from 0 to 9 and then A to F, representing values from 0 to 15.

For instance, the byte 10110110 in binary can be represented as B6 in hexadecimal. Each byte is represented by two hexadecimal digits.

Converting between binary, hexadecimal, and decimal can be useful in computer programming and data representation. It involves straightforward techniques for converting numbers from one base to another. While binary to hexadecimal conversion is relatively simple (as each group of 4 binary digits corresponds to a single hexadecimal digit), converting decimal to hexadecimal or vice versa may require a bit more effort.

## An Assemblage of Memory
As we awaken each morning, memory fills in the gaps, reminding us of where we are, what we did the day before, and what lies ahead for the day. Human memory may not be the epitome of orderliness, often taking us on tangents and leading to unrelated thoughts. It's not infallible either, leading to the invention of writing to compensate for its shortcomings.

Writing, paper, magnetic tape, and even telegraph relays are all mediums used to store information, allowing us to access it later. One fascinating element of memory is the flip-flop, a circuit capable of storing a single bit of information. With a flip-flop, we can store 1 bit, and once we grasp this concept, we can expand to store 2, 3, or more bits.

The focus of this chapter is on assembling multiple 1-bit latches into a RAM (Random Access Memory) array. A 1-bit latch, also known as a flip-flop, can hold a single bit of information. By connecting eight 1-bit latches, we create an 8-bit latch capable of storing 8 bits (1 byte) of data. However, we desire the ability to save eight separate 1-bit values at different times and examine them collectively using only one Data Out signal.

To achieve this, we introduce the concept of an 8-Line-to-1-Line Data Selector, which enables us to choose one of eight inputs and route it to the output based on three select inputs. Using this selector, we can determine the output of one specific 1-bit latch at a time.

We then proceed to construct an 8 x 1 RAM array, capable of storing eight separate 1-bit values, making it a read/write memory or RAM. Each latch in the array can be individually accessed and written to by changing the Address inputs.

Memory capacity in a RAM array is directly related to the number of Address inputs. A 1-bit latch requires no Address inputs, allowing only one value to be stored. With one Address input, the capacity increases to two values, and with two Address inputs, it becomes four, and so on.

By connecting 64K x 8 RAM arrays, each holding 65,536 bytes, we can create a memory of larger capacity. The "K" in 64K denotes 1024, reflecting the convenient approximation used for memory sizes, even though it's not strictly accurate in the metric system.

The chapter also discusses various memory sizes, such as kilobytes (KB), megabytes (MB), gigabytes (GB), terabytes (TB), petabytes (PB), and exabytes (EB). Each is approximately 1024 times larger than its predecessor due to the binary nature of memory.

A control panel is introduced to manage memory effectively, featuring switches to input addresses and data and a Write switch to store values in memory. The panel also includes a Takeover switch to allow other circuits to access the memory when open.

## Automation
In the world of computing, automation plays a central role in revolutionizing various industries and processes. Automation refers to the use of machines, computers, or robotic devices to perform tasks that were previously done by humans. The goal of automation is to increase efficiency, accuracy, and productivity while reducing manual labor and human error.

One of the earliest examples of automation can be traced back to the Industrial Revolution, where machines were introduced to replace manual labor in factories. The use of steam engines and automated looms allowed for mass production, transforming industries and economies. However, it is in the realm of computing that automation has truly flourished.

In the digital age, computers have become indispensable tools for automation. They can execute repetitive tasks at incredible speed and with remarkable precision. This ability has led to significant advancements in various fields, from manufacturing and logistics to finance and healthcare.

Automated systems are now commonplace in modern life. From self-checkout machines in supermarkets to robotic arms on assembly lines, automation has reshaped how we produce and consume goods. In the realm of data analysis, algorithms and artificial intelligence (AI) have enabled automated decision-making and predictive modeling, driving insights and efficiency in businesses and research.

However, automation also raises important questions and challenges. As machines become more autonomous, ethical considerations arise regarding the impact on jobs, privacy, and human decision-making. Striking the right balance between human control and automated efficiency remains a subject of ongoing debate.

Despite the challenges, automation continues to evolve and expand its reach. With advancements in robotics, machine learning, and the Internet of Things (IoT), we can expect to witness even greater levels of automation in the future. The promise of improved efficiency, enhanced safety, and innovative solutions drives the relentless pursuit of automation across industries.


## From Abaci to Chips
In the ever-evolving world of computing, the journey from ancient abaci to modern microchips has been an awe-inspiring tale of ingenuity and progress. The path to the digital marvels we enjoy today began with the ancient civilizations, who crafted the abacus—a simple counting tool with beads strung on rods. Little did they know that their humble invention would pave the way for the sophisticated technology of the future.

As the centuries passed, calculating devices evolved, leading to the invention of mechanical calculators, punch card machines, and vacuum tube computers. However, it was the breakthrough of the transistor that sparked the true revolution. These tiny semiconductor devices, introduced in the mid-20th century, replaced the bulky and power-hungry vacuum tubes, setting the stage for a new era in computing.

Transistors marked the birth of integrated circuits, with multiple transistors and other components etched onto a single piece of silicon, packed into a small package called a chip. The capabilities of these early chips were relatively modest, but they represented a profound shift towards miniaturization and efficiency.

The chapter delves into the two prominent chip families of the time: TTL (transistor-transistor logic) and CMOS (complementary metal-oxide semiconductor). While TTL chips offered faster performance but consumed more power, CMOS chips proved to be more power-efficient, enabling battery-powered applications.

The narrative explores the historic moment when Intel introduced the 4004 microprocessor—the world's first computer on a chip. Though modest by today's standards, this 4-bit processor was a breakthrough that set the stage for the rapid advancement of microprocessors in the years to come.

As we journey through the progression from abaci to chips, we witness the ever-accelerating pace of technological advancement. From humble beginnings, computing has evolved into a vast ecosystem of interconnected devices, revolutionizing every aspect of modern life. The quest for faster, smaller, and more efficient chips continues to drive innovation, paving the way for an uncertain yet exciting future.


## Two Classic Microprocessors
In the world of computing history, two microprocessors stand as iconic symbols of the early digital age: the Intel 8080 and the Motorola 6800. These microprocessors paved the way for the modern computing landscape and played a significant role in the development of personal computers.

The Intel 8080, introduced in 1974, was the brainchild of a team led by Federico Faggin, who later co-founded Zilog. This 8-bit microprocessor revolutionized the industry by providing the foundation for the first personal computer, the Altair 8800. With its 16-bit address bus, the 8080 could access up to 64 KB of memory, making it a powerful and versatile chip for its time. The 8080 boasted a set of instructions that allowed loading, storing, arithmetic operations, branching, and calling subroutines.

On the other side of the spectrum was the Motorola 6800, released around the same time as the 8080. Designed by the Motorola team, this 8-bit microprocessor offered a different architecture and instruction set but had similar capabilities. The 6800 featured a 16-bit index register, akin to the 8080's HL register pair, allowing for efficient address calculations.

One crucial distinction between the two chips was their approach to storing multibyte values. The 8080 used the little-endian method, storing the least-significant byte first, while the 6800 adopted the big-endian method, storing the most-significant byte first. This difference in byte ordering led to compatibility challenges when sharing information between systems based on these processors.

Despite their differences, both the 8080 and 6800 laid the foundation for subsequent microprocessors. The 8080's legacy continued with the Zilog Z-80, which added more instructions and was used in the Radio Shack TRS-80 Model 1. Meanwhile, the 6800 series found a place in the Apple Macintosh, and later, Macintosh computers transitioned to the PowerPC microprocessor.

Over the years, microprocessors have seen a rapid evolution in terms of data width, instruction sets, and performance. The increase in transistor count, as predicted by Moore's Law, has enabled modern processors to implement advanced features such as pipelining and caching, enhancing their processing speed and efficiency.

While the 8080 and 6800 are no longer at the forefront of computing technology, their contributions to the digital world continue to be remembered and celebrated. As we delve further into the realm of computer programming, we will explore how text and information are encoded in memory, taking us back to the basics of reading and writing for computers.


## ASCII and a Cast of Characters
The American Standard Code for Information Interchange, better known as ASCII, is a fundamental character encoding system that has played a crucial role in the development of computers and digital communication. In the world of computing, where everything is represented as a series of ones and zeros, ASCII provides a standardized way to represent text characters and symbols.

ASCII is a 7-bit code, meaning it uses 7 binary digits (bits) to represent each character. This allows for a total of 128 unique characters to be encoded, including letters (both uppercase and lowercase), digits, punctuation marks, and various control characters. The basic ASCII character set covers the English alphabet, digits from 0 to 9, common punctuation marks, and a few control characters.

The encoding scheme works by assigning numeric values to each character, ranging from 0 to 127. For example, the letter "A" is represented by the binary value 01000001, which corresponds to the decimal value 65 in ASCII. Similarly, "a" is represented by 01100001, which corresponds to decimal 97. The uppercase and lowercase letters are distinguished by the difference in their binary representation's third bit.

Aside from graphic characters, ASCII also includes control characters, which serve specific functions rather than having visual representations. These control characters, such as the carriage return and line feed, are used for formatting text on devices like printers and teletypewriters.

Though ASCII has been the dominant character encoding standard for many years, it does have limitations. Notably, it is primarily tailored to the English language and lacks support for many non-Latin alphabets and special characters used in other languages. To address these issues, various extensions and alternative character encoding systems have been developed, such as Unicode, which is a 16-bit code capable of representing characters from various languages.

Despite its limitations, ASCII remains an integral part of computer systems and communication protocols. Its historical significance and universal use have solidified its place as one of the foundational pillars of modern computing.


## Get on the Bus
In the world of computer architecture, "the bus" refers to a critical communication pathway that connects various components within a computer system. The bus is like a busy highway, enabling the smooth flow of data between the microprocessor, memory, and other peripherals. Understanding how the bus works is crucial for anyone looking to grasp the inner workings of a computer.

Imagine the bus as a series of wires running through the computer's circuitry, acting as a data highway. The data signals travel on these wires, shuttling between different parts of the computer as needed. However, not all data travels at the same speed or in the same format. Some components use different voltage levels or logic to represent 0s and 1s, making it essential to ensure compatibility among devices.

One integral component of the bus is the tri-state output, which plays a pivotal role in managing data flow. This output can have three states: logical 0, logical 1, or a third state that resembles an unconnected pin. When a device's output is in the third state, it effectively disconnects from the bus, allowing another device to control the data signals. This feature ensures that only one component is driving the bus at a given time, avoiding conflicts and data corruption.

Additionally, memory boards play a crucial role in computer architecture, allowing the microprocessor to store and retrieve data quickly. Among various types of memory, static random access memory (SRAM) and dynamic random access memory (DRAM) are the most common. SRAM retains its contents as long as it has power, while DRAM requires periodic refreshing to maintain data integrity.

On the display front, the cathode-ray tube (CRT) takes center stage as the most common output device for computers. A CRT operates by rapidly sweeping a single beam of light across the screen, creating an image composed of scan lines. Television signals, for instance, use 60 scan lines per second, making the image appear continuous and flicker-free to the human eye.

The video display adapter is responsible for controlling the video signal sent to the CRT, converting it into a grid of pixels. Each pixel represents a small dot on the screen, and the number of pixels and colors that can be displayed depend on the video adapter's resolution and capabilities. For text-based displays, characters are represented by pixel patterns, and the display adapter must have its own RAM to store the contents of the display.

Another crucial aspect of computer architecture is the keyboard interface. Each key on the keyboard acts as a switch that generates a unique code, known as the scan code, when pressed. The scan code allows the microprocessor to determine which key was pressed and respond accordingly.

Furthermore, long-term storage devices, such as floppy disks and hard disks, are essential for preserving data even when the power is turned off. Unlike volatile memory, storage is non-volatile and retains data until it's deliberately erased or overwritten. Magnetic storage, in the form of disks, has become prevalent due to its fast access times and higher capacity.

Understanding the bus and its role in connecting various components is fundamental to comprehending computer architecture. The smooth operation of this data highway ensures efficient data flow, making it possible for computers to carry out complex tasks and provide the functionalities we rely on in our daily lives.

## The Operating System
The operating system serves as the backbone of any computer, orchestrating the interactions between hardware, software, and the user. At its core, an operating system is responsible for managing resources, executing programs, and providing a user-friendly interface.

One essential aspect of an operating system is its file system, which allows data to be stored, organized, and accessed efficiently. Think of it as a virtual file cabinet, where each file has a little tab indicating its name. The historical significance of operating systems for early 8-bit microprocessors is highlighted by CP/M (Control Program for Micros), created by Gary Kildall in the mid-1970s for the Intel 8080 microprocessor. CP/M was instrumental in laying the foundation for modern operating systems.

CP/M utilized a straightforward file system, where each file on the disk was identified by a name stored along with the file's data. Files did not need to occupy consecutive sectors on the disk, which made it flexible and efficient. The simplicity of CP/M's file system, known as 8.3, allowed for easy file management but had limitations in terms of the number of files it could handle.

Later, MS-DOS (Microsoft Disk Operating System) emerged as a dominant operating system, featuring a file system known as the File Allocation Table (FAT). MS-DOS abandoned CP/M's naming convention and introduced hierarchical directories, allowing files to be organized into nested folders. MS-DOS quickly became popular due to its compatibility with IBM PCs and its relatively minimal interference with programming, making it a favored choice among developers.

While CP/M and MS-DOS laid the groundwork for operating systems, UNIX took a different approach. Developed at Bell Telephone Laboratories in the early 1970s, UNIX was designed to be portable, adaptable to various computer systems. It introduced the concept of multitasking, allowing multiple users to interact with the computer simultaneously, making it a true multitasking operating system. UNIX's philosophy of using text files as a common denominator and its flexible utility chain made it beloved among computer programmers.

In recent years, the Free Software Foundation (FSF) and the GNU project have contributed to UNIX-compatible utilities and tools, including Linux, which has gained significant popularity. Meanwhile, large and sophisticated systems like Apple Macintosh and Microsoft Windows have integrated graphics and visually-rich displays, making applications more user-friendly.

The evolution of operating systems continues to shape the way we interact with computers, striving for greater efficiency, user-friendliness, and adaptability to meet the ever-changing demands of technology.

## Fixed Point, Floating Point
Numbers are numbers, and in most of our daily lives, we drift casually between whole numbers, fractions, and percentages. We buy half a carton of eggs and pay 8 ¼ percent sales tax with money earned getting time-and-a-half for working 2 ¾ hours overtime. Most people are fairly comfortable—if not necessarily proficient—with numbers such as these. We can even hear a statistic like "the average American household has 2.6 people" without gasping in horror at the widespread mutilation that must have occurred to achieve this.

Yet this interchange between whole numbers and fractions isn't so casual when it comes to computer memory. Yes, everything is stored in computers in the form of bits, which means that everything is stored as binary numbers. But some kinds of numbers are definitely easier to express in terms of bits than others.

We began using bits to represent what mathematicians call the positive whole numbers and what computer programmers call the positive integers. We've also seen how two's complements allow us to represent negative integers in a way that eases the addition of positive and negative numbers. The table below shows the range of positive integers and two's-complement integers for 8, 16, and 32 bits of storage.

Number of Bits | Range of Positive Integers | Range of Two's-Complement Integers
------------ | --------------- | ---------------
8 | 0 through 255 | -128 through 127
16 | 0 through 65,535 | -32,768 through 32,767
32 | 0 through 4,294,967,295 | -2,147,483,648 through 2,147,483,647

But that's where we stopped. Beyond whole numbers, mathematicians also define rational numbers as those numbers that can be represented as a ratio of two whole numbers. This ratio is also referred to as a fraction. For example, ¾ is a rational number because it's the ratio of 3 and 4. We can also write this number in decimal fraction, or just decimal, form: 0.75.

When we write a number in decimal form, it really indicates a fraction. You'll recall from previous chapter that in a decimal number system, digits to the left of the decimal point are multiples of integral powers of ten. Similarly, digits to the right of the decimal point are multiples of negative powers of ten. The author used the example 42,705.684, showing first that it's equal to 4 x 10,000 + 2 x 1000 + 7 x 100 + 0 x 10 + 5 x 1 + 6 ÷ 10 + 8 ÷ 100 + 4 ÷ 1000. Notice the division signs. Then the author showed how you can write this sequence without any division: 4 x 10,000 + 2 x 1000 + 7 x 100 + 0 x 10 + 5 x 1 + 6 x 0.1 + 8 x 0.01 + 4 x 0.001. And finally, here's the number using powers of ten: 4 x 10^4 + 2 x 10^3 + 7 x 10^2 + 0 x 10^1 + 5 x 10^0 + 6 x 10^-1 + 8 x 10^-2 + 4 x 10^-3.

Some rational numbers aren't so easily represented as decimals, the most obvious being 1/3. If you divide 3 into 1, you'll find that 1/3 is equal to 0.3333333333333333333333333333333333333333333333333333…and on and on and on. It's common to write this more concisely with a little bar over the 3 to indicate that the digit repeats forever: 0.3̅.

Even though writing 1/3 as a decimal fraction is a bit awkward, it's still a rational number because it's the ratio of two integers. Similarly, √2 is irrational, as it can't be expressed as the ratio of two integers, and its decimal fraction continues indefinitely without any repetition or pattern.

The evolution of floating-point notation has played a crucial role in modern computing, allowing for precise representation of a wide range of numbers, from very large to very small, and handling complex mathematical operations effectively. While fixed-point notation is suitable for specific scenarios, such as financial calculations involving dollars and cents, floating-point notation provides the flexibility and accuracy needed in scientific, engineering, and other computational domains. With the integration of hardware support, floating-point calculations have become faster and more efficient, empowering computer programmers to tackle complex tasks and advance the realms of science and technology.

Floating-point notation continues to be a fundamental aspect of modern computing, enriching our ability to process vast amounts of data, simulate complex systems, and solve intricate mathematical problems. As computers evolve further and scientific demands grow more sophisticated, floating-point arithmetic will remain an indispensable tool in shaping our understanding of the world and exploring new frontiers in technology and research.

## Languages High and Low
In the world of programming, a multitude of languages has emerged, each designed with specific purposes and paradigms. This chapter delves into the evolution of programming languages, exploring the differences between "high-level" and "low-level" languages and the impact they've had on the field of computing.

High-level languages, as the name suggests, abstract the complexities of computer hardware and offer a more user-friendly interface for developers. One such language, ALGOL, serves as an archetype for understanding the foundations of programming concepts. ALGOL, with its ancestry dating back to the late 1950s, influenced many popular general-purpose languages that followed.

ALGOL's successor, COBOL, catered to business applications and introduced support for reading records and generating reports. Another language, PL/I, attempted to combine the best of ALGOL, FORTRAN, and COBOL but never achieved the same popularity. On the other hand, BASIC (Beginner's All-purpose Symbolic Instruction Code) rapidly gained traction due to its simplicity and early implementations on home computers, eventually leading to Microsoft's success.

Pascal, developed in the late 1960s, was particularly popular among IBM PC programmers, especially in its Turbo Pascal form. It also served as a foundation for Ada, a language developed for the United States Department of Defense.

One of the most influential languages, C, emerged in the early 1970s. Renowned for its portability, C became closely associated with the UNIX operating system after it was rewritten in the language. C's concise syntax, resembling processor instructions, earned it the label of a "high-level assembly language."

Apart from the dominant ALGOL-like languages, this chapter also explores less conventional languages like LISP, designed for artificial intelligence work, and APL, which utilized a unique set of symbols to perform operations on arrays of numbers.

Despite the ongoing evolution and emergence of object-oriented languages, the ALGOL-inspired languages continue to hold their dominance, driving innovation in programming and shaping the world of computing as we know it.


## The Graphical Revolution
In the early days of computing, computers communicated with their users through text-based interfaces, displaying lines of characters on monochrome screens. However, a major transformation was on the horizon - the Graphical Revolution. This chapter delves into the pivotal moment when graphical interfaces changed the way people interacted with computers, leading to the widespread adoption of personal computers and revolutionizing the entire computing landscape.

The graphical revolution began with the advent of the graphical user interface (GUI) and the introduction of iconic elements such as windows, icons, menus, and pointers, commonly known as WIMP. These elements provided users with a visually intuitive way to interact with computers, as opposed to the traditional command-line interfaces that required memorizing complex commands.

The chapter explores the pioneers of graphical interfaces, such as Douglas Engelbart, who showcased groundbreaking concepts like the mouse and interactive text editing in the 1960s. However, it wasn't until Xerox PARC (Palo Alto Research Center) in the 1970s that the graphical revolution truly took off. Xerox PARC's innovations, including the Alto computer with its graphical interface, laid the foundation for modern GUIs.

One of the key contributions of Xerox PARC was the development of the bitmap display, which enabled the representation of graphics and text as individual pixels. This breakthrough allowed for more sophisticated and visually appealing interfaces. The chapter delves into the technical aspects of bitmap graphics and how they influenced the evolution of computer graphics.

The narrative then explores how companies like Apple and Microsoft took inspiration from Xerox PARC's work and popularized graphical interfaces. Apple's Lisa and Macintosh computers brought GUIs to the mainstream market, while Microsoft's Windows operating system later became the dominant player in the GUI space.

The rise of graphical capabilities didn't stop at desktop computers. The chapter also covers the development of computer graphics in other domains, such as video games, CAD programs, and animation. As computers became more powerful, they could process and display complex graphics, leading to immersive and visually stunning virtual worlds.

The graphical revolution also extended to printers and scanners, enabling the reproduction of digital images in high resolution and full color. The chapter delves into the concepts of vector graphics and raster graphics, explaining their respective applications in engineering design and representing real-world images.

In addition to graphics, the chapter explores how sound and video were digitized and integrated into multimedia applications. It discusses the development of audio codecs, MIDI synthesizers, and the compression techniques used to store and transmit audio and video data efficiently.
