# The Self-Assembling Brain: How Neural Networks Grow Smarter

Peter Robin Hiesinger (2021)

***

In the book "The Self-Assembling Brain: How Neural Networks Grow Smarter" by Peter Robin Hiesinger, the author explores the intersection of developmental neurobiology and artificial intelligence (AI). The book discusses the information problem in both fields and the potential insights that can be gained from studying brain development. It emphasises the importance of algorithmic growth and simulation in understanding the complexities of biological systems.

The book covers ten seminars that address various aspects of brain development and AI. These seminars touch on information encoding, algorithmic growth, the role of genetic mutations in behaviour, molecular mechanisms in neural network assembly, and the levels of problems in studying neural circuits. The seminars also delve into the connection between brain development and AI, cognitive biases, and the transfer of brain information.

Throughout the book, the author highlights the shared questions and challenges developmental neurobiologists and AI researchers face. It emphasises the need for cross-disciplinary collaboration and knowledge exchange to advance our understanding of intelligent systems.

The book acknowledges the differences between biological and artificial neural networks, particularly regarding their growth processes. While artificial neural networks typically start with predetermined structures and learn from data, biological neural networks undergo continuous self-organisation and adaptation. The author explores the implications of these differences for achieving artificial general intelligence.

Overall, "The Self-Assembling Brain" offers a perspective on brain development and its connection to AI. It aims to bridge the gap between developmental neurobiology and AI research, providing insights into the complexities of information encoding and algorithmic growth in biological and artificial systems.

***

### Introduction

Developmental neurobiologists and AI researchers aim to understand intelligent systems, studying interconnected components like neurons in biological brains or artificial neural networks. However, the usefulness of the analogy between natural and artificial systems is debated.

Both fields grapple with the problem of determining the necessary information for brain wiring or creating AI. Biological systems encode limited explicit information in the genome for developmental growth, while ANNs learn from input data through training.

Learning in both systems occurs step by step and relies on ordered processes, but there are differences. AI research traditionally favours formal, symbol-processing logic over neural networks.

AI's historical trajectory bypasses biological details to create intelligent systems efficiently. However, understanding information encoding and transformation in brain development can offer insights and shortcuts for AI development.

This book takes readers on the author's neurobiologist journey, exploring brain development and the shared questions and challenges encountered in pursuing intelligent systems.

#### The Perspective of Neurobiological Information

Brain wiring, a remarkable self-assembly process, lacks a predetermined blueprint encoded in genes. Instead, genetic information guides brain growth and development, activating specific genome segments. This dynamic process relies on energy and features a continuous feedback loop between the genome and its products. Analytical methods alone cannot decipher the genetic code; it requires executing the program itself over time and considering various influences. One must embrace the temporal and energetic aspects of gene information unfolding rather than relying solely on static analytical methods to grasp the intricacies of brain wiring.

#### The Perspective of Algorithmic Information

Like Rule 110, cellular automata reveal how simple rules generate complex patterns and computations. However, describing the complete information of such systems requires specifying the rules and their repeated application. This approach, called Kolmogorov complexity, provides an alternative way to describe endpoint information.

While cellular automata have low algorithmic information content, understanding their behaviour necessitates infinite iterations. Rule 110 shows unpredictable information unfolding but lacks changing rules and dynamic outcomes, making it unsuitable for modelling biological systems.

Biological growth, guided by the genetic code, is more intricate, involving dynamic rules and stochastic processes. Mathematical calculation alone cannot capture the unpredictable unfolding of information. Algorithmic growth and simulation are needed to comprehend the complex interplay of genetic and environmental factors.

This chapter hypothesises that biological systems require algorithmic growth or simulation to grasp information unfolding entirely. By adopting this perspective, the author aims to explore the nature of knowledge in biological systems and its connection to algorithmic growth.

#### A Shared Perspective

The genetic code plays a crucial role in brain development, providing algorithmic information for the growth program. However, it does not directly determine brain connectivity. Molecular mechanisms encoded in genes offer insights into brain development processes, and gene mutations can impact behaviour by affecting brain function.

Understanding the relationship between molecular mechanisms and higher-order behaviour involves exploring self-assembly and algorithmic growth in the brain. Deciphering the outcome of developmental changes resulting from genetic mutations presents an information problem.

Self-assembly is well-studied in biological systems and artificial life but has received less attention in artificial intelligence (AI). AI research has primarily focused on neural networks and deep learning, which do not rely on a genetic code for growth. Instead, artificial neural networks (ANNs) initialise with random connections and learn from data using simple rules.

The author has organised a seminar series and workshop to bridge the gap between artificial life, AI, and developmental neurobiology. These events foster cross-disciplinary learning and encourage collaboration among developmental geneticists, neuroscientists, robotics engineers, and AI researchers.

The chapter presents ten fictional dialogues among experts from these fields. These dialogues discuss pertinent questions and challenges at the intersection of these disciplines. The author seeks to address the fascinating opportunities and obstacles in this study area by facilitating knowledge exchange and collaboration.


#### The Ten Seminars

The seminar series explores the development and function of neural networks in developmental neurobiology and AI research. It addresses shared questions and challenges between the fields. The first seminar highlights the initial struggles and emphasises the tension between rigorous outcomes and random manipulations.

Seminar 2 examines different types of information in biological and artificial neural networks, referencing cellular automata and genetic encoding. Seminar 3 focuses on randomness in algorithmic growth and the synaptic specificity paradox. Seminar 4 discusses autonomous agents in self-assembly and their relevance to neural network development.

The second session explores the connection between genetic mutations and behavioural reprogramming. Seminar 5 explores animal behaviour reprogramming and the absence of a single gene responsible for complex behaviours. Seminar 6 discusses molecular mechanisms in neural network assembly, while Seminar 7 addresses the levels problem in studying neural circuits.

The third session covers transitions in neural networks and their interactions. Seminar 8 discusses developmental self-assembly and the distinctions between engineered and biological networks. Seminar 9 examines artificial neural network limitations and the importance of information storage and retrieval in algorithmic growth.

Seminar 10 focuses on cognitive bias, probabilistic information in neural networks, and the transfer of brain information. Throughout the workshops, questions arise about intelligence, simulating human intelligence, and understanding information storage and retrieval for artificial intelligence.

#### On Common Ground

The chapter targets readers interested in developmental biology or AI. While it may need more depth for experts in either field, it aims to find common ground and explore questions related to information encoding. The author does not provide comprehensive overviews of the fields' history or achievements.

The author highlights the overlooked implications of unpredictable information unfolding and challenges intuitive thinking in both fields. Some readers may find the concepts oversimplified or lacking detailed coverage of molecular mechanisms.

The author focuses on understanding how neural networks develop intelligence, with historical perspectives woven in. AI researchers specialising in Artificial Life (ALife) may initially find it less helpful due to different perspectives. Developmental neurobiologists may prioritise practical applications of deep learning over the author's air on brain development in AI.

### The Present and the Past

#### Seminar 1: The Deeply Engrained Worship of Tidy-Looking Dichotomies

The seminar series and workshop explore the intertwined history of brain development and AI, focusing on information encoding in neural networks. Key figures and their contributions are discussed, including the neuron doctrine, electron microscopy, plasticity, resonance theory, and the McCulloch-Pitts neuron.

The dichotomy between formal logic and probability-based approaches in AI research and the resurgence of neural networks in the 1980s is explored. Developmental neurobiology challenges rigid chemoaffinity, emphasising developmental flexibility and the role of timing in network formation.

The precise mapping of axons proposed by Sperry is contrasted with the discovery of repulsion gradients and the role of target-derived molecules in guiding axons. The "neat vs scruffy" debate in AI and the challenges of understanding artificial neural network solutions are addressed.

Understanding brain wiring and applying biological knowledge to AI remains a central objective. Ongoing research investigates molecular mechanisms, wiring specificity, and knowledge exchange between AI and biology to unravel the nature of intelligence and brain development.

## Session 1: Algorithmic Growth

### Information? What Information?

#### Seminar 2: From Algorithmic Growth to Endpoint Information

Cellular automata, such as the Game of Life, exemplify the interplay of determinism and unpredictability. Rule 110 is Turing-complete and demonstrates infinite complexity. Algorithmic growth is crucial in understanding complex biological structures where rules are iteratively applied. Precision in describing these structures may only sometimes be necessary, as some aspects may be irrelevant or noisy.

The relationship between information and energy is explored through information theory and entropy. Biological systems, like dendritic trees and transcription factor cascades, undergo algorithmic growth processes. Knowledge in biology is categorised as instructive or permissive, with composite instructions playing critical roles in decision-making during development.

The depth of relevant information depends on the desired level of analysis. Genetic code and cellular automata rules use the metaphor of a "code," but neurons' continuously changing states challenge this metaphor. Understanding the rules of algorithmic growth is prioritised over specific genes or molecules, emphasising the importance of simple rules and algorithms in biological systems.

The search for simple rules and algorithms may seem reductionist, but it offers valuable insights into the beauty and functions of complex patterns in biology.

### Noise and Relevant Information

#### Seminar 3: From Randomness to Precision

Bacteria like *E. coli* display chemotaxis, moving towards sugar sources through sensory perception and motor activation. By performing a biased random walk, *E. coli* effectively navigates towards higher sugar concentrations without knowing their locations.

Noise and randomness are crucial in algorithmic growth processes, contributing to developing neurons and synaptic connections. Dendritic self-avoidance and Notch signalling facilitate synaptic specificity and pattern formation. Synaptic promiscuity and noise-based growth mechanisms increase the target area for axons to find correct partners.

The brain's growth algorithm involves synaptic pruning and partner selection mechanisms. Different visual systems, such as those in flies, frogs, and fish, rely on precise wiring and the sorting of axon terminals through developmental processes. Synaptic promiscuity, noise, and variability contribute to adaptive brain wiring and can be influenced by the environment and stochastic development.

Using noise in brain development creates order and variability, with implications for understanding neurodevelopmental disorders and monozygotic twins. Studying the role of noise is essential for comprehending the intricate processes of brain development.


### Autonomous Agents and Local Rules

#### Seminar 4: From Local Rules to Robustness

The German premier league soccer games demonstrate predictable and reproducible outcomes at a seasonal level, yet individual game results can be highly unpredictable. Autonomous agents, such as players and coaches, follow local rules, continuously interacting and adjusting their actions based on others' behaviour. This behaviour contributes to the game's overall unpredictability and robustness, reflecting the self-organisation principles seen in biological systems.

The concept of dynamic normality applies to autonomous agents functioning within an environment ideally suited to their abilities, contributing to the higher-order organisation of the system. This principle of self-organisation is observed in various levels, from individual neurons to complex social insects. Autonomous agents operate independently, producing emergent and complex behaviours without a central controller.

Ross Ashby's work in cybernetics and self-organising dynamic systems provides valuable insights into understanding the behaviour of autonomous agents. Feedback mechanisms and adaptability play key roles in complex systems. Manipulations at the genetic level can reveal underlying rules governing neuronal conduct. However, predicting the effects of these manipulations can be challenging due to the complex interactions among autonomous agents and their impact on the overall system. Evolutionary processes offer insights into programming the development of neural networks through genetic mutations, helping researchers understand the evolution and functioning of neural networks in biological systems.

## Session 2: Of Players and Rules

### The Benzer Paradox

#### Seminar 5: From Molecular Mechanisms to Evolutionary Programming

The "little King Kong" midge in Taiwan showcases the evolutionary arms race between predators and prey, with inconspicuous traits aiding survival. Cordyceps, a fungus, reprogram the behaviour of infected ants to aid in dispersing its spores, possibly by reprogramming their brains. Genes play a crucial role in encoding information for organisms and the brains of various species.

Single mutations in an organism's genome can have profound inheritable and selectable effects on behaviour. Neurogenetics studies have identified specific gene mutations contributing to meaningful and heritable behavioural changes. Mutations in specialised genes like "period" and "fruitless" can disrupt daily behaviour rhythm and impact sexual behaviour in fruit flies.

Understanding the effects of mutations on behaviour is challenging due to the unpredictability of algorithmic growth and the involvement of multiple genes. Forward genetic screens have revealed mutations in various genes, but comprehending their precise effects is complex. The genetic basis of behaviour can involve multiple genes with diverse functions, including unexpected ones like those involved in cytoskeletal regulation.

To fully understand the effects of mutations on complex behaviours, it is crucial to consider context, molecular functions, and the coordinated events involved in the algorithmic part. While general principles explain broad concepts in biology, a detailed understanding of specific processes is necessary for a comprehensive experience.

### The Molecules That Could

#### Seminar 6: From Chemoaffinity to the Virtues of Permissiveness 

Researchers delve into the genetic basis of behaviour, exploring genes involved in complex neuronal circuitry and various cellular processes. Friedrich Bonhoeffer's in vitro choice assays led to the discovery of ephrins and Eph receptors, shedding light on repulsive signalling during axon guidance and the intricate processes of neural development.

The relative positioning of axons and environmental cues are crucial for topographic mapping. The discovery of repulsive ephrins supported the chemoaffinity theory proposed by Roger Sperry, highlighting the role of chemical tags in target definitions. However, the information problem associated with chemoaffinity theory remains challenging for researchers.

Sydney Brenner's work with Caenorhabditis elegans provided insights into synaptic specificity and the developmental flexibility of the nervous system. The mapping of the C. elegance wiring diagram revealed the influence of developmental mechanisms and positional effects on shaping its neural connectivity. Brenner's forward genetic screens identified mutants, including the netrin-encoding gene unc-6, which plays a complex and context-dependent role in axonal growth and guidance.

Composite instructions involving multiple factors and their interactions drive brain wiring rather than relying solely on instructive signals. These composite instructions specify axonal growth and guidance during the algorithmic growth of the neural network. Understanding the functions of genes and factors involved in brain wiring proves intricate, with multiple influences shaping the process.

### The Levels Problem

#### Seminar 7: From Genes to Cells to Circuits

Researchers like Gerry Rubin at the Janelia Research Campus are studying the connectome of model organisms, such as fruit flies, to unravel how brain wiring generates behaviour. Optogenetics, a technique that uses light-activated proteins to manipulate neuronal activity, has revolutionised the field, allowing scientists to selectively deactivate or activate neurons and gain insights into neural circuit workings.

Neural circuits, interconnected neurons with specific functions, rely on compensatory and feedback mechanisms to maintain their integrity and position. While circuit function is primarily studied at the cellular level, research on circuit assembly has focused chiefly on genetic perturbations rather than cellular ones due to the lack of acute cellular-level perturbation technologies. Bridging the "levels problem" between molecular processes and higher-level phenomena, such as behaviour, remains challenging, as multiple factors and interactions influence complex traits.

Understanding the genetic basis of complex traits requires considering context, the penetrance of mutations, and the interplay between genetic variation and environmental factors. Genetic studies face challenges in predicting the outcome of genetic variations due to the complex interactions of multiple genes and nonadditive effects. The example of monarch butterfly migration highlights the involvement of numerous genes with different functions and the influence of genetic changes and environmental factors on trait manifestation.

Focusing on the rules and players at the behavioural level allows researchers to describe complex systems while considering important genetic details. Describing the developmental algorithms, functional properties of neurons, and neuronal connectivity independently of molecular specifics enables simplifications for modelling neural networks and implementing practical simulations. Understanding the intricate details and complexity of the growth process is crucial for accurately simulating and emulating neural networks in artificial intelligence research.

## Session 3: Brain Development and Artificial Intelligence

### You Are Your History

#### Seminar 8: From Development to Function

The design of the human eye raises questions about how such a complex structure could have evolved through natural selection. For example, the eye's intricate neural network in the retina is a result of its evolutionary history rather than being designed specifically for its current function. Developmental constraints also play a significant role, with complex organs like the eye-shaped by genetic and molecular processes influenced by environmental conditions and genetic interactions.

In the brain, the organisation and functionality of cortical columns, a topic of debate in neuroscience, are influenced by the interplay of evolutionary and developmental processes. The brain's structure and network of interconnected neurons have evolved, contributing to our cognitive abilities. Memory and information processing in the brain is dynamic and integrated into the neural network itself. Memories are stored as algorithmic rules and are influenced by new experiences and associations, interacting with one another and changing over time.

Training and learning are crucial in shaping the brain's network and memory formation. The brain's plasticity allows for modifications in synaptic connections and the embedding new associations. Gerald Edelman's neural group selection and reentry theory proposes a selectional system based on changing synaptic strengths and subcircuit synchronisation. The growth and function of neural networks, including memory formation and brain function, are governed by algorithmic rules, with no centralised master regulator dictating every aspect of brain development and information processing.

### Self-Assembly versus "Build First, Train Later."

#### Seminar 9: From Algorithmic Growth to Artificial Intelligence 

Self-organisation is a phenomenon in which order emerges from initially disordered components through local interactions and feedback mechanisms. Artificial neural networks (ANNs) traditionally lack self-organising features but can exhibit them by integrating feedback mechanisms and introducing noise. In contrast, biological neural networks continuously self-organise throughout their development and adapt to environmental and physiological demands, contributing to their flexibility and robustness.

Algorithmic growth is fundamental in developing biological neural networks involving sequential self-organising steps. ANNs typically skip the growth process and start with predetermined structures refined through training. The interplay between action and function is a distinctive characteristic of biological networks, enabling ongoing adaptation and self-organisation, unlike ANNs trained for specific tasks.

Artificial general intelligence, aimed at mimicking multiple aspects of human intelligence, requires understanding algorithmic growth and neural network development. Defining intelligence is challenging, and current AI approaches often focus on functional aspects rather than the dynamic growth observed in biological systems. Trade-offs between development speed, complexity, and capabilities must be considered in AI development, and exploring algorithmically growing networks and evolutionary programming can lead to more biologically inspired AI systems.

### Final Frontiers: Beloved Beliefs and the AI-Brain Interface

#### Seminar 10: From Cognitive Bias to Whole Brain Emulation
 
Herbert Simon's work challenged traditional notions of decision-making and introduced the concept of heuristics. Heuristics are cognitive shortcuts that guide decision-making using approximations and rules of thumb. Simon's research demonstrated the potential for machines to think, learn, and create, expanding their abilities over time.

AI has always been interdisciplinary, attracting experts from various fields. Amos Tversky and Daniel Kahneman built upon Simon's work by identifying cognitive biases and judgment heuristics employed by the human brain. Cognitive biases are intrinsic features of algorithmic growth resulting from the brain's reliance on patterns and previous experiences.

While shortcuts may be unfeasible in biological systems' algorithmic growth, reasonable shortcuts can still provide valuable insights into natural and artificial neural networks. However, it is essential to recognise that shortcuts may overlook important details and properties of neural networks. Achieving whole-brain emulation and artificial general intelligence requires overcoming challenges related to brain-machine interfaces and achieving seamless integration between humans and AI.

The future of technology and AI may involve nonlinear advancements beyond current engineering achievements. Understanding how information is stored in the brain requires a holistic approach that acknowledges the complexity of biological systems and may require engineering solutions to comprehend fully. Learning and adaptation in the cortex require time and progression on its terms, and there are no shortcuts in these processes.
